#!/usr/bin/env python3
"""
ONNX ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å‹•ä½œæ¤œè¨¼ãƒ„ãƒ¼ãƒ«
ç•°ãªã‚‹ç’°å¢ƒãƒ»è¨­å®šã§ã®ONNXãƒ¢ãƒ‡ãƒ«äº’æ›æ€§æ¤œè¨¼
"""
import time
import numpy as np
import onnxruntime as ort
from pathlib import Path
import sys
import platform
import subprocess
import json
from typing import Dict, List, Tuple

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def get_system_info():
    """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±åé›†"""
    print("=== ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±åé›† ===")
    
    info = {
        'platform': platform.platform(),
        'machine': platform.machine(),
        'processor': platform.processor(),
        'python_version': platform.python_version(),
        'onnxruntime_version': ort.__version__,
        'available_providers': ort.get_available_providers(),
    }
    
    # NumPyæƒ…å ±
    info['numpy_version'] = np.__version__
    
    # CPUã‚³ã‚¢æ•°
    try:
        import psutil
        info['cpu_count'] = psutil.cpu_count()
        info['memory_gb'] = psutil.virtual_memory().total / 1024**3
    except ImportError:
        info['cpu_count'] = 'N/A'
        info['memory_gb'] = 'N/A'
    
    # GPUæƒ…å ±ï¼ˆå¯èƒ½ã§ã‚ã‚Œã°ï¼‰
    try:
        import torch
        if torch.cuda.is_available():
            info['cuda_available'] = True
            info['cuda_version'] = torch.version.cuda
            info['gpu_count'] = torch.cuda.device_count()
            info['gpu_name'] = torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else 'N/A'
        else:
            info['cuda_available'] = False
    except ImportError:
        info['cuda_available'] = 'Unknown'
    
    for key, value in info.items():
        print(f"  {key}: {value}")
    
    return info

def validate_onnx_model_structure():
    """ONNX ãƒ¢ãƒ‡ãƒ«æ§‹é€ æ¤œè¨¼"""
    print("\n=== ONNX ãƒ¢ãƒ‡ãƒ«æ§‹é€ æ¤œè¨¼ ===")
    
    onnx_path = project_root / "exports/onnx/yolov7n_mezzopiano.onnx"
    
    if not onnx_path.exists():
        print(f"âŒ ONNXãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {onnx_path}")
        return False
    
    try:
        import onnx
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ»æ¤œè¨¼
        print("ğŸ“ ONNXãƒ¢ãƒ‡ãƒ«æ§‹é€ æ¤œè¨¼ä¸­...")
        model = onnx.load(str(onnx_path))
        onnx.checker.check_model(model)
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—
        model_info = {
            'file_size_mb': onnx_path.stat().st_size / 1024 / 1024,
            'ir_version': model.ir_version,
            'producer_name': model.producer_name,
            'producer_version': model.producer_version,
            'graph_input_count': len(model.graph.input),
            'graph_output_count': len(model.graph.output),
            'graph_node_count': len(model.graph.node)
        }
        
        print(f"âœ… ONNXãƒ¢ãƒ‡ãƒ«æ§‹é€ : æ­£å¸¸")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {model_info['file_size_mb']:.1f}MB")
        print(f"   IR Version: {model_info['ir_version']}")
        print(f"   Producer: {model_info['producer_name']} {model_info['producer_version']}")
        print(f"   å…¥åŠ›æ•°: {model_info['graph_input_count']}")
        print(f"   å‡ºåŠ›æ•°: {model_info['graph_output_count']}")
        print(f"   ãƒãƒ¼ãƒ‰æ•°: {model_info['graph_node_count']}")
        
        return model_info
        
    except ImportError:
        print("âš ï¸ onnxãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åŸºæœ¬æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return {'file_size_mb': onnx_path.stat().st_size / 1024 / 1024}
    except Exception as e:
        print(f"âŒ ONNX ãƒ¢ãƒ‡ãƒ«æ§‹é€ æ¤œè¨¼å¤±æ•—: {e}")
        return False

def test_provider_compatibility():
    """å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼äº’æ›æ€§ãƒ†ã‚¹ãƒˆ"""
    print("\n=== ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼äº’æ›æ€§ãƒ†ã‚¹ãƒˆ ===")
    
    onnx_path = project_root / "exports/onnx/yolov7n_mezzopiano.onnx"
    
    # ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
    test_providers = [
        (['CPUExecutionProvider'], 'CPUåŸºæœ¬å®Ÿè¡Œ'),
        (['AzureExecutionProvider', 'CPUExecutionProvider'], 'Azureå®Ÿè¡Œ'),
    ]
    
    # CUDAåˆ©ç”¨å¯èƒ½ã§ã‚ã‚Œã°ãƒ†ã‚¹ãƒˆ
    available = ort.get_available_providers()
    if 'CUDAExecutionProvider' in available:
        test_providers.append((['CUDAExecutionProvider', 'CPUExecutionProvider'], 'CUDA GPUå®Ÿè¡Œ'))
    
    if 'TensorrtExecutionProvider' in available:
        test_providers.append((['TensorrtExecutionProvider', 'CPUExecutionProvider'], 'TensorRTå®Ÿè¡Œ'))
    
    results = {}
    
    for providers, description in test_providers:
        print(f"\nğŸ” ãƒ†ã‚¹ãƒˆä¸­: {description}")
        
        try:
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            session = ort.InferenceSession(str(onnx_path), providers=providers)
            
            # å…¥åŠ›æº–å‚™
            input_name = session.get_inputs()[0].name
            dummy_input = np.random.randn(1, 3, 640, 640).astype(np.float32)
            
            # æ¨è«–ãƒ†ã‚¹ãƒˆ
            start_time = time.time()
            outputs = session.run(None, {input_name: dummy_input})
            end_time = time.time()
            
            inference_time = (end_time - start_time) * 1000  # ms
            
            results[description] = {
                'success': True,
                'providers_requested': providers,
                'providers_used': session.get_providers(),
                'inference_time_ms': inference_time,
                'output_count': len(outputs),
                'output_shapes': [out.shape for out in outputs]
            }
            
            print(f"âœ… æˆåŠŸ: {inference_time:.1f}ms")
            print(f"   ä½¿ç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {', '.join(session.get_providers())}")
            
        except Exception as e:
            results[description] = {
                'success': False,
                'providers_requested': providers,
                'error': str(e)
            }
            print(f"âŒ å¤±æ•—: {e}")
    
    return results

def test_input_variations():
    """å…¥åŠ›ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
    print("\n=== å…¥åŠ›ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ ===")
    
    onnx_path = project_root / "exports/onnx/yolov7n_mezzopiano.onnx"
    
    try:
        session = ort.InferenceSession(str(onnx_path))
        input_name = session.get_inputs()[0].name
        
        # ãƒ†ã‚¹ãƒˆå…¥åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³
        test_cases = [
            ('æ­£å¸¸å…¥åŠ›', np.random.randn(1, 3, 640, 640).astype(np.float32)),
            ('ã‚¼ãƒ­å…¥åŠ›', np.zeros((1, 3, 640, 640), dtype=np.float32)),
            ('æœ€å¤§å€¤å…¥åŠ›', np.ones((1, 3, 640, 640), dtype=np.float32)),
            ('ãƒ©ãƒ³ãƒ€ãƒ ç¯„å›²å…¥åŠ›', np.random.uniform(0, 1, (1, 3, 640, 640)).astype(np.float32)),
            ('æ­£è¦åŒ–æ¸ˆã¿å…¥åŠ›', (np.random.randn(1, 3, 640, 640) * 0.5 + 0.5).astype(np.float32)),
        ]
        
        results = {}
        
        for test_name, test_input in test_cases:
            print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆ: {test_name}")
            
            try:
                start_time = time.time()
                outputs = session.run(None, {input_name: test_input})
                end_time = time.time()
                
                inference_time = (end_time - start_time) * 1000
                
                # å‡ºåŠ›æ¤œè¨¼
                output_valid = all(
                    not np.isnan(out).any() and not np.isinf(out).any() 
                    for out in outputs
                )
                
                results[test_name] = {
                    'success': True,
                    'inference_time_ms': inference_time,
                    'output_valid': output_valid,
                    'output_shapes': [out.shape for out in outputs],
                    'output_ranges': [(float(np.min(out)), float(np.max(out))) for out in outputs[:3]]  # æœ€åˆã®3ã¤ã®ã¿
                }
                
                status = "âœ… æœ‰åŠ¹" if output_valid else "âš ï¸ ç„¡åŠ¹å€¤æ¤œå‡º"
                print(f"   {status}: {inference_time:.1f}ms")
                
            except Exception as e:
                results[test_name] = {
                    'success': False,
                    'error': str(e)
                }
                print(f"   âŒ å¤±æ•—: {e}")
        
        return results
        
    except Exception as e:
        print(f"âŒ å…¥åŠ›ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return {}

def test_concurrent_inference():
    """ä¸¦è¡Œæ¨è«–ãƒ†ã‚¹ãƒˆ"""
    print("\n=== ä¸¦è¡Œæ¨è«–ãƒ†ã‚¹ãƒˆ ===")
    
    onnx_path = project_root / "exports/onnx/yolov7n_mezzopiano.onnx"
    
    try:
        import threading
        import queue
        
        session = ort.InferenceSession(str(onnx_path))
        input_name = session.get_inputs()[0].name
        
        # ä¸¦è¡Œæ¨è«–ãƒ†ã‚¹ãƒˆè¨­å®š
        num_threads = 4
        inferences_per_thread = 10
        
        results_queue = queue.Queue()
        
        def worker():
            """ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰"""
            thread_results = []
            
            for i in range(inferences_per_thread):
                dummy_input = np.random.randn(1, 3, 640, 640).astype(np.float32)
                
                try:
                    start_time = time.time()
                    outputs = session.run(None, {input_name: dummy_input})
                    end_time = time.time()
                    
                    thread_results.append({
                        'success': True,
                        'inference_time_ms': (end_time - start_time) * 1000
                    })
                    
                except Exception as e:
                    thread_results.append({
                        'success': False,
                        'error': str(e)
                    })
            
            results_queue.put(thread_results)
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰å®Ÿè¡Œ
        print(f"ğŸ”„ {num_threads}ã‚¹ãƒ¬ãƒƒãƒ‰ã§ä¸¦è¡Œæ¨è«–ãƒ†ã‚¹ãƒˆä¸­...")
        
        threads = []
        start_time = time.time()
        
        for _ in range(num_threads):
            thread = threading.Thread(target=worker)
            threads.append(thread)
            thread.start()
        
        for thread in threads:
            thread.join()
        
        total_time = time.time() - start_time
        
        # çµæœé›†è¨ˆ
        all_results = []
        while not results_queue.empty():
            all_results.extend(results_queue.get())
        
        successful_inferences = [r for r in all_results if r['success']]
        failed_inferences = [r for r in all_results if not r['success']]
        
        if successful_inferences:
            inference_times = [r['inference_time_ms'] for r in successful_inferences]
            avg_time = np.mean(inference_times)
            max_time = np.max(inference_times)
            min_time = np.min(inference_times)
            
            concurrent_result = {
                'total_inferences': len(all_results),
                'successful_inferences': len(successful_inferences),
                'failed_inferences': len(failed_inferences),
                'total_time_seconds': total_time,
                'avg_inference_time_ms': avg_time,
                'max_inference_time_ms': max_time,
                'min_inference_time_ms': min_time,
                'throughput_fps': len(successful_inferences) / total_time
            }
            
            print(f"âœ… ä¸¦è¡Œæ¨è«–ãƒ†ã‚¹ãƒˆå®Œäº†")
            print(f"   æˆåŠŸ: {len(successful_inferences)}/{len(all_results)}")
            print(f"   å¹³å‡æ¨è«–æ™‚é–“: {avg_time:.1f}ms")
            print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {concurrent_result['throughput_fps']:.1f} FPS")
            
            return concurrent_result
        else:
            print(f"âŒ å…¨ã¦ã®ä¸¦è¡Œæ¨è«–ãŒå¤±æ•—")
            return {'error': 'å…¨æ¨è«–å¤±æ•—'}
        
    except ImportError:
        print("âš ï¸ threadingãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä¸¦è¡Œãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return {'skipped': 'threading not available'}
    except Exception as e:
        print(f"âŒ ä¸¦è¡Œæ¨è«–ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return {'error': str(e)}

def generate_compatibility_report(system_info: Dict, model_info: Dict, provider_results: Dict, 
                                 input_results: Dict, concurrent_result: Dict, output_dir: Path):
    """äº’æ›æ€§æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    print(f"\n=== äº’æ›æ€§æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ ===")
    
    report_path = output_dir / "cross_platform_compatibility_report.md"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# YOLOv7n ONNX ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ äº’æ›æ€§ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"**ç”Ÿæˆæ—¥æ™‚**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        f.write("## ğŸ–¥ï¸ ãƒ†ã‚¹ãƒˆç’°å¢ƒ\n\n")
        f.write(f"- **ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ **: {system_info['platform']}\n")
        f.write(f"- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: {system_info['machine']}\n")
        f.write(f"- **Python**: {system_info['python_version']}\n")
        f.write(f"- **ONNX Runtime**: {system_info['onnxruntime_version']}\n")
        f.write(f"- **NumPy**: {system_info['numpy_version']}\n")
        f.write(f"- **CPU**: {system_info['cpu_count']}ã‚³ã‚¢\n")
        f.write(f"- **ãƒ¡ãƒ¢ãƒª**: {system_info['memory_gb']:.1f}GB\n")
        f.write(f"- **CUDAåˆ©ç”¨å¯èƒ½**: {system_info['cuda_available']}\n")
        f.write(f"- **åˆ©ç”¨å¯èƒ½ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼**: {', '.join(system_info['available_providers'])}\n\n")
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
        if model_info:
            f.write("## ğŸ“‹ ãƒ¢ãƒ‡ãƒ«æƒ…å ±\n\n")
            f.write(f"- **ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º**: {model_info['file_size_mb']:.1f}MB\n")
            if 'ir_version' in model_info:
                f.write(f"- **IR Version**: {model_info['ir_version']}\n")
                f.write(f"- **Producer**: {model_info['producer_name']} {model_info['producer_version']}\n")
                f.write(f"- **å…¥åŠ›æ•°**: {model_info['graph_input_count']}\n")
                f.write(f"- **å‡ºåŠ›æ•°**: {model_info['graph_output_count']}\n")
                f.write(f"- **ãƒãƒ¼ãƒ‰æ•°**: {model_info['graph_node_count']}\n")
            f.write("\n")
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼äº’æ›æ€§
        f.write("## âš™ï¸ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼äº’æ›æ€§\n\n")
        f.write("| ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ | ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ | æ¨è«–æ™‚é–“(ms) | å®Ÿéš›ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ |\n")
        f.write("|-------------|-----------|-------------|------------------|\n")
        
        for provider_desc, result in provider_results.items():
            if result['success']:
                actual_providers = ', '.join(result['providers_used'])
                f.write(f"| {provider_desc} | âœ… æˆåŠŸ | {result['inference_time_ms']:.1f} | {actual_providers} |\n")
            else:
                f.write(f"| {provider_desc} | âŒ å¤±æ•— | - | - |\n")
        
        # å…¥åŠ›ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        f.write("\n## ğŸ§ª å…¥åŠ›ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ\n\n")
        f.write("| ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ | ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ | æ¨è«–æ™‚é–“(ms) | å‡ºåŠ›æœ‰åŠ¹æ€§ |\n")
        f.write("|-------------|-----------|-------------|----------|\n")
        
        for test_name, result in input_results.items():
            if result['success']:
                validity = "âœ… æœ‰åŠ¹" if result['output_valid'] else "âš ï¸ ç„¡åŠ¹å€¤"
                f.write(f"| {test_name} | âœ… æˆåŠŸ | {result['inference_time_ms']:.1f} | {validity} |\n")
            else:
                f.write(f"| {test_name} | âŒ å¤±æ•— | - | - |\n")
        
        # ä¸¦è¡Œæ¨è«–
        f.write("\n## ğŸ”„ ä¸¦è¡Œæ¨è«–ãƒ†ã‚¹ãƒˆ\n\n")
        if 'error' not in concurrent_result and 'skipped' not in concurrent_result:
            f.write(f"- **ç·æ¨è«–æ•°**: {concurrent_result['total_inferences']}\n")
            f.write(f"- **æˆåŠŸæ¨è«–**: {concurrent_result['successful_inferences']}\n")
            f.write(f"- **å¤±æ•—æ¨è«–**: {concurrent_result['failed_inferences']}\n")
            f.write(f"- **ç·å®Ÿè¡Œæ™‚é–“**: {concurrent_result['total_time_seconds']:.2f}ç§’\n")
            f.write(f"- **å¹³å‡æ¨è«–æ™‚é–“**: {concurrent_result['avg_inference_time_ms']:.1f}ms\n")
            f.write(f"- **æœ€å¤§æ¨è«–æ™‚é–“**: {concurrent_result['max_inference_time_ms']:.1f}ms\n")
            f.write(f"- **æœ€å°æ¨è«–æ™‚é–“**: {concurrent_result['min_inference_time_ms']:.1f}ms\n")
            f.write(f"- **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: {concurrent_result['throughput_fps']:.1f} FPS\n")
        elif 'skipped' in concurrent_result:
            f.write(f"âš ï¸ ã‚¹ã‚­ãƒƒãƒ—: {concurrent_result['skipped']}\n")
        else:
            f.write(f"âŒ å¤±æ•—: {concurrent_result['error']}\n")
        
        # ç·åˆè©•ä¾¡
        f.write("\n## ğŸ¯ ç·åˆè©•ä¾¡\n\n")
        
        success_count = sum(1 for r in provider_results.values() if r['success'])
        total_provider_tests = len(provider_results)
        
        input_success_count = sum(1 for r in input_results.values() if r['success'])
        total_input_tests = len(input_results)
        
        f.write(f"### äº’æ›æ€§ã‚¹ã‚³ã‚¢\n")
        f.write(f"- **ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼äº’æ›æ€§**: {success_count}/{total_provider_tests} ({success_count/total_provider_tests*100:.1f}%)\n")
        f.write(f"- **å…¥åŠ›è€æ€§**: {input_success_count}/{total_input_tests} ({input_success_count/total_input_tests*100:.1f}%)\n")
        
        if success_count == total_provider_tests and input_success_count == total_input_tests:
            f.write(f"- **ç·åˆè©•ä¾¡**: âœ… å„ªç§€ - å…¨ãƒ†ã‚¹ãƒˆé€šé\n")
        elif success_count >= total_provider_tests * 0.8:
            f.write(f"- **ç·åˆè©•ä¾¡**: âš ï¸ è‰¯å¥½ - ä¸€éƒ¨åˆ¶é™ã‚ã‚Š\n")
        else:
            f.write(f"- **ç·åˆè©•ä¾¡**: âŒ è¦æ”¹å–„ - é‡è¦ãªå•é¡Œã‚ã‚Š\n")
        
        f.write("\n### æ¨å¥¨äº‹é …\n")
        
        # CPUå°‚ç”¨ç’°å¢ƒã§ã®æ¨å¥¨
        cpu_provider_success = any(
            r['success'] and 'CPUExecutionProvider' in r['providers_used'] 
            for r in provider_results.values()
        )
        
        if cpu_provider_success:
            f.write("- âœ… CPUç’°å¢ƒã§ã®å‹•ä½œç¢ºèªæ¸ˆã¿\n")
        else:
            f.write("- âŒ CPUç’°å¢ƒã§ã®å‹•ä½œã«å•é¡Œã‚ã‚Š\n")
        
        # GPUåˆ©ç”¨æ¨å¥¨
        gpu_provider_success = any(
            r['success'] and any(p in r['providers_used'] for p in ['CUDAExecutionProvider', 'TensorrtExecutionProvider'])
            for r in provider_results.values()
        )
        
        if gpu_provider_success:
            f.write("- âœ… GPU ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ©ç”¨å¯èƒ½\n")
        else:
            f.write("- âš ï¸ GPU ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æœªå¯¾å¿œã¾ãŸã¯æœªãƒ†ã‚¹ãƒˆ\n")
        
        f.write("\n")
    
    print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
    return report_path

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸ” ONNX ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ äº’æ›æ€§æ¤œè¨¼é–‹å§‹")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = project_root / "analysis/cross_platform"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        # 1. ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±åé›†
        system_info = get_system_info()
        
        # 2. ONNX ãƒ¢ãƒ‡ãƒ«æ§‹é€ æ¤œè¨¼
        model_info = validate_onnx_model_structure()
        if not model_info:
            return False
        
        # 3. ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼äº’æ›æ€§ãƒ†ã‚¹ãƒˆ
        provider_results = test_provider_compatibility()
        
        # 4. å…¥åŠ›ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ
        input_results = test_input_variations()
        
        # 5. ä¸¦è¡Œæ¨è«–ãƒ†ã‚¹ãƒˆ
        concurrent_result = test_concurrent_inference()
        
        # 6. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_path = generate_compatibility_report(
            system_info, model_info, provider_results,
            input_results, concurrent_result, output_dir
        )
        
        print(f"\nğŸ‰ ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ¤œè¨¼å®Œäº†!")
        print(f"ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ¤œè¨¼å¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)