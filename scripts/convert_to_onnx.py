#!/usr/bin/env python3
"""
YOLOv7n PyTorch â†’ ONNX å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
GPUå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ONNXå½¢å¼ã«å¤‰æ›ã—ã¦æ¨è«–æœ€é©åŒ–
"""
import torch
import os
import sys
import time
from pathlib import Path
import numpy as np

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def load_trained_model():
    """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰"""
    print("=== å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ ===")
    
    # æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ¤œç´¢
    checkpoint_dir = project_root / "runs/train/gpu_finetuning/checkpoints"
    if not checkpoint_dir.exists():
        raise FileNotFoundError(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {checkpoint_dir}")
    
    checkpoint_files = list(checkpoint_dir.glob("*.ckpt"))
    if not checkpoint_files:
        raise FileNotFoundError("ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé¸æŠ
    latest_checkpoint = max(checkpoint_files, key=lambda x: x.stat().st_mtime)
    print(f"ğŸ“ ä½¿ç”¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {latest_checkpoint}")
    
    try:
        # Lightningå½¢å¼ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ­ãƒ¼ãƒ‰
        checkpoint = torch.load(latest_checkpoint, map_location='cpu')
        print(f"âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
        print(f"   ã‚¨ãƒãƒƒã‚¯: {checkpoint.get('epoch', 'N/A')}")
        print(f"   ã‚¹ãƒ†ãƒƒãƒ—: {checkpoint.get('global_step', 'N/A')}")
        
        # state_dictå–å¾—
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
            print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {len(state_dict)}")
        else:
            raise ValueError("state_dictãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
        return state_dict, latest_checkpoint
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        raise

def create_model_for_export():
    """ONNXå¤‰æ›ç”¨ãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
    print("\n=== ONNXå¤‰æ›ç”¨ãƒ¢ãƒ‡ãƒ«ä½œæˆ ===")
    
    try:
        from yolo.model.yolo import YOLO
        from yolo.config.config import ModelConfig
        from omegaconf import OmegaConf
        
        # v9-sè¨­å®šãƒ­ãƒ¼ãƒ‰
        model_config_path = project_root / "yolo/config/model/v9-s.yaml"
        if not model_config_path.exists():
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_config_path}")
        
        # è¨­å®šãƒ­ãƒ¼ãƒ‰
        config_dict = OmegaConf.load(model_config_path)
        model_cfg = OmegaConf.structured(ModelConfig(**config_dict))
        
        print(f"âœ… è¨­å®šãƒ­ãƒ¼ãƒ‰: {model_config_path}")
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ– (mezzopianoç”¨1ã‚¯ãƒ©ã‚¹)
        model = YOLO(model_cfg, class_num=1)
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ä½œæˆæˆåŠŸ: {type(model)}")
        
        # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
        model.eval()
        
        return model
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        raise

def convert_to_onnx(model, state_dict, output_path):
    """PyTorch â†’ ONNX å¤‰æ›å®Ÿè¡Œ"""
    print(f"\n=== ONNXå¤‰æ›å®Ÿè¡Œ ===")
    
    try:
        # å­¦ç¿’æ¸ˆã¿é‡ã¿é©ç”¨
        print("å­¦ç¿’æ¸ˆã¿é‡ã¿é©ç”¨ä¸­...")
        model.load_state_dict(state_dict, strict=False)
        print("âœ… é‡ã¿é©ç”¨å®Œäº†")
        
        # ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆ (YOLOv7næ¨™æº–ã‚µã‚¤ã‚º)
        dummy_input = torch.randn(1, 3, 640, 640)
        print(f"ğŸ¯ å…¥åŠ›ã‚µã‚¤ã‚º: {dummy_input.shape}")
        
        # ONNXå¤‰æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        export_params = {
            'input_names': ['input'],
            'output_names': ['output'],
            'dynamic_axes': {
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            },
            'opset_version': 11,
            'do_constant_folding': True,
            'export_params': True,
        }
        
        print(f"ğŸ“¤ ONNXå¤‰æ›é–‹å§‹: {output_path}")
        start_time = time.time()
        
        # ONNXå¤‰æ›å®Ÿè¡Œ
        torch.onnx.export(
            model,
            dummy_input,
            str(output_path),
            **export_params
        )
        
        conversion_time = time.time() - start_time
        print(f"âœ… ONNXå¤‰æ›å®Œäº†! ({conversion_time:.2f}ç§’)")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        file_size_mb = output_path.stat().st_size / (1024 * 1024)
        print(f"ğŸ“ ONNXãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.1f} MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ ONNXå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def verify_onnx_model(onnx_path):
    """ONNX ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼"""
    print(f"\n=== ONNX ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ ===")
    
    try:
        import onnx
        import onnxruntime as ort
        
        # ONNX ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼
        print("ONNXãƒ¢ãƒ‡ãƒ«æ§‹é€ æ¤œè¨¼ä¸­...")
        onnx_model = onnx.load(str(onnx_path))
        onnx.checker.check_model(onnx_model)
        print("âœ… ONNXãƒ¢ãƒ‡ãƒ«æ§‹é€ OK")
        
        # ONNX Runtime ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
        print("ONNX Runtime ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
        session = ort.InferenceSession(str(onnx_path))
        print("âœ… ONNX Runtime ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
        
        # å…¥å‡ºåŠ›æƒ…å ±è¡¨ç¤º
        print("\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
        for input_tensor in session.get_inputs():
            print(f"  å…¥åŠ›: {input_tensor.name} {input_tensor.shape} {input_tensor.type}")
        
        for output_tensor in session.get_outputs():
            print(f"  å‡ºåŠ›: {output_tensor.name} {output_tensor.shape} {output_tensor.type}")
        
        # ãƒ†ã‚¹ãƒˆæ¨è«–å®Ÿè¡Œ
        print("\nãƒ†ã‚¹ãƒˆæ¨è«–å®Ÿè¡Œä¸­...")
        test_input = np.random.randn(1, 3, 640, 640).astype(np.float32)
        
        start_time = time.time()
        outputs = session.run(None, {'input': test_input})
        inference_time = time.time() - start_time
        
        print(f"âœ… ãƒ†ã‚¹ãƒˆæ¨è«–æˆåŠŸ!")
        print(f"   æ¨è«–æ™‚é–“: {inference_time*1000:.1f}ms")
        print(f"   å‡ºåŠ›å½¢çŠ¶: {[out.shape for out in outputs]}")
        
        return True, session
        
    except ImportError as e:
        print(f"âš ï¸ ONNX/ONNX Runtimeæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: {e}")
        print("pip install onnx onnxruntime ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
        return False, None
    except Exception as e:
        print(f"âŒ ONNXæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return False, None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš€ YOLOv7n ONNXå¤‰æ›é–‹å§‹")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = project_root / "exports" / "onnx"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_path = output_dir / "yolov7n_mezzopiano.onnx"
    
    try:
        # 1. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        state_dict, checkpoint_path = load_trained_model()
        
        # 2. å¤‰æ›ç”¨ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        model = create_model_for_export()
        
        # 3. ONNXå¤‰æ›å®Ÿè¡Œ
        success = convert_to_onnx(model, state_dict, output_path)
        if not success:
            return False
        
        # 4. ONNX ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼
        verify_success, session = verify_onnx_model(output_path)
        
        if verify_success:
            print(f"\nğŸ‰ ONNXå¤‰æ›å®Œäº†!")
            print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")
            print(f"ğŸ“Š å…ƒãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {checkpoint_path}")
            print(f"ğŸ”§ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ONNX Runtimeæœ€é©åŒ–")
        else:
            print(f"\nâš ï¸ ONNXå¤‰æ›ã¯å®Œäº†ã—ã¾ã—ãŸãŒã€æ¤œè¨¼ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ONNXå¤‰æ›å¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)