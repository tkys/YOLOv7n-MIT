#!/usr/bin/env python3
"""
å­¦ç¿’éç¨‹åˆ†æãƒ»ãƒ­ã‚®ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Lossæ¨ç§»ã€ç²¾åº¦å¤‰åŒ–ã€å­¦ç¿’ç‡ãªã©ã‚’è©³ç´°ã«åˆ†æãƒ»å¯è¦–åŒ–
"""
import os
import re
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import numpy as np
from datetime import datetime

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)

def parse_lightning_logs(log_dir):
    """PyTorch Lightning ãƒ­ã‚°ã‹ã‚‰è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º"""
    print(f"=== ãƒ­ã‚°è§£æé–‹å§‹: {log_dir} ===")
    
    metrics = {
        'epoch': [],
        'train_loss': [],
        'box_loss': [],
        'dfl_loss': [],
        'bce_loss': [],
        'learning_rate': [],
        'val_ap_50': [],
        'val_ap_50_95': [],
        'val_recall': [],
        'timestamp': []
    }
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ¢ç´¢
    log_files = []
    for ext in ['*.log', '*.txt', 'events.out.tfevents*']:
        log_files.extend(list(Path(log_dir).rglob(ext)))
    
    print(f"ç™ºè¦‹ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(log_files)}")
    
    # å„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ
    for log_file in log_files:
        try:
            print(f"è§£æä¸­: {log_file}")
            parse_single_log(log_file, metrics)
        except Exception as e:
            print(f"âš ï¸ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼ {log_file}: {e}")
    
    return metrics

def parse_single_log(log_file, metrics):
    """å˜ä¸€ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æ"""
    with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
        content = f.read()
    
    # ã‚¨ãƒãƒƒã‚¯æƒ…å ±æŠ½å‡º
    epoch_pattern = r'Epoch\s+(\d+)'
    epochs = re.findall(epoch_pattern, content)
    
    # Losså€¤æŠ½å‡º
    loss_patterns = {
        'BoxLoss': r'BoxLoss[^\d]*([0-9\.]+)',
        'DFLLoss': r'DFLLoss[^\d]*([0-9\.]+)', 
        'BCELoss': r'BCELoss[^\d]*([0-9\.]+)'
    }
    
    for loss_type, pattern in loss_patterns.items():
        values = re.findall(pattern, content)
        print(f"  {loss_type}: {len(values)}å€‹ã®å€¤ã‚’ç™ºè¦‹")
    
    # APå€¤æŠ½å‡ºï¼ˆè¡¨å½¢å¼ï¼‰
    ap_pattern = r'AP @\s+\.5.*?(\d+\.?\d*)'
    ap_values = re.findall(ap_pattern, content)
    print(f"  AP@0.5å€¤: {len(ap_values)}å€‹ç™ºè¦‹")
    
    return metrics

def extract_tensorboard_logs(log_dir):
    """TensorBoardå½¢å¼ãƒ­ã‚°æŠ½å‡º"""
    try:
        from tensorboard.backend.event_processing import event_accumulator
        
        tb_logs = list(Path(log_dir).rglob('events.out.tfevents*'))
        if not tb_logs:
            print("TensorBoardãƒ­ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {}
            
        print(f"TensorBoardãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {len(tb_logs)}å€‹")
        
        metrics = {}
        for tb_log in tb_logs:
            ea = event_accumulator.EventAccumulator(str(tb_log))
            ea.Reload()
            
            # åˆ©ç”¨å¯èƒ½ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
            scalars = ea.Tags()['scalars']
            print(f"åˆ©ç”¨å¯èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {scalars}")
            
            for scalar in scalars:
                scalar_events = ea.Scalars(scalar)
                metrics[scalar] = {
                    'steps': [e.step for e in scalar_events],
                    'values': [e.value for e in scalar_events],
                    'timestamps': [e.wall_time for e in scalar_events]
                }
        
        return metrics
        
    except ImportError:
        print("TensorBoardæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã‚¹ã‚­ãƒƒãƒ—")
        return {}

def parse_checkpoint_info(checkpoint_dir):
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å­¦ç¿’çŠ¶æ…‹åˆ†æ"""
    print(f"\n=== ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆåˆ†æ: {checkpoint_dir} ===")
    
    checkpoint_files = list(Path(checkpoint_dir).glob('*.ckpt'))
    if not checkpoint_files:
        print("ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return {}
    
    print(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {len(checkpoint_files)}å€‹")
    
    try:
        import torch
        
        info = {}
        for ckpt_file in checkpoint_files:
            print(f"è§£æä¸­: {ckpt_file.name}")
            
            try:
                checkpoint = torch.load(ckpt_file, map_location='cpu')
                
                info[ckpt_file.name] = {
                    'epoch': checkpoint.get('epoch', 'N/A'),
                    'global_step': checkpoint.get('global_step', 'N/A'),
                    'lr_schedulers': len(checkpoint.get('lr_schedulers', [])),
                    'optimizer_states': len(checkpoint.get('optimizer_states', [])),
                    'model_size_mb': ckpt_file.stat().st_size / (1024*1024),
                    'keys': list(checkpoint.keys())[:10]  # æœ€åˆã®10ã‚­ãƒ¼
                }
                
            except Exception as e:
                print(f"  âš ï¸ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        return info
        
    except ImportError:
        print("PyTorchæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã‚¹ã‚­ãƒƒãƒ—")
        return {}

def create_training_visualization(metrics, output_dir):
    """å­¦ç¿’éç¨‹å¯è¦–åŒ–"""
    print(f"\n=== å¯è¦–åŒ–ä½œæˆ: {output_dir} ===")
    
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    if not any(metrics.values()):
        print("âš ï¸ å¯è¦–åŒ–ç”¨ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        return
    
    # 1. Lossæ¨ç§»ã‚°ãƒ©ãƒ•
    plt.figure(figsize=(15, 10))
    
    plt.subplot(2, 3, 1)
    loss_types = ['box_loss', 'dfl_loss', 'bce_loss']
    for loss_type in loss_types:
        if metrics.get(loss_type):
            plt.plot(metrics[loss_type], label=loss_type.replace('_', ' ').title())
    plt.title('Lossæ¨ç§»')
    plt.xlabel('Step/Epoch')
    plt.ylabel('Losså€¤')
    plt.legend()
    plt.grid(True)
    
    # 2. å­¦ç¿’ç‡æ¨ç§»
    plt.subplot(2, 3, 2)
    if metrics.get('learning_rate'):
        plt.plot(metrics['learning_rate'], label='Learning Rate', color='red')
        plt.title('å­¦ç¿’ç‡æ¨ç§»')
        plt.xlabel('Step/Epoch')
        plt.ylabel('Learning Rate')
        plt.yscale('log')
        plt.grid(True)
    
    # 3. ç²¾åº¦æ¨ç§»
    plt.subplot(2, 3, 3)
    ap_metrics = ['val_ap_50', 'val_ap_50_95']
    for ap_metric in ap_metrics:
        if metrics.get(ap_metric):
            plt.plot(metrics[ap_metric], label=ap_metric.replace('_', ' ').upper())
    plt.title('ç²¾åº¦æ¨ç§»')
    plt.xlabel('Epoch')
    plt.ylabel('Average Precision')
    plt.legend()
    plt.grid(True)
    
    # 4. ç·åˆLoss
    plt.subplot(2, 3, 4)
    if metrics.get('train_loss'):
        plt.plot(metrics['train_loss'], label='Train Loss', color='blue')
        plt.title('ç·åˆLossæ¨ç§»')
        plt.xlabel('Step/Epoch')
        plt.ylabel('Total Loss')
        plt.grid(True)
    
    # 5. Recallæ¨ç§»
    plt.subplot(2, 3, 5)
    if metrics.get('val_recall'):
        plt.plot(metrics['val_recall'], label='Validation Recall', color='green')
        plt.title('Recallæ¨ç§»')
        plt.xlabel('Epoch')
        plt.ylabel('Recall')
        plt.grid(True)
    
    # 6. ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
    plt.subplot(2, 3, 6)
    summary_data = []
    summary_labels = []
    
    for metric_name, values in metrics.items():
        if values and isinstance(values[0], (int, float)):
            summary_data.append([np.min(values), np.mean(values), np.max(values)])
            summary_labels.append(metric_name.replace('_', ' ').title())
    
    if summary_data:
        summary_df = pd.DataFrame(summary_data, 
                                columns=['Min', 'Mean', 'Max'],
                                index=summary_labels)
        
        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º
        sns.heatmap(summary_df.T, annot=True, fmt='.3f', cmap='viridis')
        plt.title('ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµ±è¨ˆã‚µãƒãƒªãƒ¼')
    
    plt.tight_layout()
    
    # ä¿å­˜
    viz_path = output_dir / 'training_analysis.png'
    plt.savefig(viz_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"âœ… å¯è¦–åŒ–ä¿å­˜: {viz_path}")

def generate_training_report(metrics, checkpoint_info, output_dir):
    """è©³ç´°å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    print(f"\n=== å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {output_dir} ===")
    
    output_dir = Path(output_dir)
    report_path = output_dir / 'training_report.md'
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"# YOLOv7n Mezzopianoå­¦ç¿’åˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¦‚è¦
        f.write("## ğŸ“Š å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¦‚è¦\n\n")
        for metric_name, values in metrics.items():
            if values and isinstance(values[0], (int, float)):
                f.write(f"### {metric_name.replace('_', ' ').title()}\n")
                f.write(f"- **ãƒ‡ãƒ¼ã‚¿æ•°**: {len(values)}\n")
                f.write(f"- **æœ€å°å€¤**: {np.min(values):.4f}\n")
                f.write(f"- **å¹³å‡å€¤**: {np.mean(values):.4f}\n")
                f.write(f"- **æœ€å¤§å€¤**: {np.max(values):.4f}\n")
                f.write(f"- **æœ€çµ‚å€¤**: {values[-1]:.4f}\n\n")
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæƒ…å ±
        f.write("## ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæƒ…å ±\n\n")
        for ckpt_name, info in checkpoint_info.items():
            f.write(f"### {ckpt_name}\n")
            f.write(f"- **ã‚¨ãƒãƒƒã‚¯**: {info.get('epoch', 'N/A')}\n")
            f.write(f"- **ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ãƒ†ãƒƒãƒ—**: {info.get('global_step', 'N/A')}\n")
            f.write(f"- **ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º**: {info.get('model_size_mb', 0):.1f} MB\n")
            f.write(f"- **ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼çŠ¶æ…‹**: {info.get('optimizer_states', 0)}å€‹\n\n")
        
        # å­¦ç¿’æˆæœã‚µãƒãƒªãƒ¼
        f.write("## ğŸ¯ å­¦ç¿’æˆæœã‚µãƒãƒªãƒ¼\n\n")
        if metrics.get('val_ap_50'):
            final_ap50 = metrics['val_ap_50'][-1]
            f.write(f"- **æœ€çµ‚AP@0.5**: {final_ap50:.3f} ({final_ap50*100:.1f}%)\n")
        
        if metrics.get('val_ap_50_95'):
            final_ap50_95 = metrics['val_ap_50_95'][-1]
            f.write(f"- **æœ€çµ‚AP@0.5:0.95**: {final_ap50_95:.3f} ({final_ap50_95*100:.1f}%)\n")
        
        if metrics.get('train_loss'):
            initial_loss = metrics['train_loss'][0]
            final_loss = metrics['train_loss'][-1]
            loss_reduction = ((initial_loss - final_loss) / initial_loss) * 100
            f.write(f"- **Losså‰Šæ¸›**: {initial_loss:.3f} â†’ {final_loss:.3f} ({loss_reduction:.1f}%å‰Šæ¸›)\n")
    
    print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸ” YOLOv7nå­¦ç¿’éç¨‹è©³ç´°åˆ†æé–‹å§‹")
    
    # æœ€æ–°ã®å­¦ç¿’çµæœã‚’ç‰¹å®š
    runs_dir = Path("runs/train")
    if not runs_dir.exists():
        print("âŒ runs/trainãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    train_dirs = [d for d in runs_dir.iterdir() if d.is_dir()]
    if not train_dirs:
        print("âŒ å­¦ç¿’çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # æœ€æ–°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆGPU Fine-tuningå„ªå…ˆï¼‰
    gpu_dirs = [d for d in train_dirs if 'gpu' in d.name.lower()]
    if gpu_dirs:
        latest_dir = max(gpu_dirs, key=lambda x: x.stat().st_mtime)
    else:
        latest_dir = max(train_dirs, key=lambda x: x.stat().st_mtime)
    
    print(f"ğŸ“‚ åˆ†æå¯¾è±¡: {latest_dir}")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    analysis_dir = Path("analysis") / latest_dir.name
    analysis_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. ãƒ­ã‚°è§£æ
    metrics = parse_lightning_logs(latest_dir)
    
    # 2. TensorBoardãƒ­ã‚°ï¼ˆã‚ã‚Œã°ï¼‰
    tb_metrics = extract_tensorboard_logs(latest_dir)
    
    # 3. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆåˆ†æ
    checkpoint_dir = latest_dir / "checkpoints"
    checkpoint_info = parse_checkpoint_info(checkpoint_dir)
    
    # 4. å¯è¦–åŒ–ä½œæˆ
    create_training_visualization(metrics, analysis_dir)
    
    # 5. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    generate_training_report(metrics, checkpoint_info, analysis_dir)
    
    print(f"\nğŸ‰ å­¦ç¿’åˆ†æå®Œäº†ï¼")
    print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {analysis_dir}")
    print(f"ğŸ“Š å¯è¦–åŒ–: {analysis_dir}/training_analysis.png")
    print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆ: {analysis_dir}/training_report.md")

if __name__ == "__main__":
    main()