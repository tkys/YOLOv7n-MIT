"""
YOLOv7n MIT Edition - åŒ…æ‹¬çš„è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ãƒ»é€Ÿåº¦ãƒ»ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®è‡ªå‹•è©•ä¾¡
"""

import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import psutil
import seaborn as sns
from jinja2 import Template

from inference.batch_images import BatchImageInference
from inference.logger import get_logger
from inference.single_image import SingleImageInference

logger = get_logger("evaluation")

class ModelEvaluator:
    """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_path: str, test_data_dir: str):
        """
        è©•ä¾¡å™¨åˆæœŸåŒ–
        
        Args:
            model_path: è©•ä¾¡ã™ã‚‹ONNXãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
            test_data_dir: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.model_path = Path(model_path)
        self.test_data_dir = Path(test_data_dir)
        
        # æ¨è«–å™¨åˆæœŸåŒ–
        self.single_inferencer = SingleImageInference(
            model_path=str(self.model_path),
            confidence_threshold=0.5
        )
        
        self.batch_inferencer = BatchImageInference(
            model_path=str(self.model_path),
            confidence_threshold=0.5,
            max_workers=4
        )
        
        logger.info(
            "ãƒ¢ãƒ‡ãƒ«è©•ä¾¡å™¨åˆæœŸåŒ–å®Œäº†",
            model_path=str(self.model_path),
            test_data_dir=str(self.test_data_dir)
        )
    
    def evaluate_accuracy(self) -> Dict:
        """ç²¾åº¦è©•ä¾¡"""
        logger.info("ç²¾åº¦è©•ä¾¡é–‹å§‹")
        
        # ãƒ†ã‚¹ãƒˆç”»åƒã§æ¨è«–å®Ÿè¡Œ
        test_images = list(self.test_data_dir.glob("*.jpg")) + \
                     list(self.test_data_dir.glob("*.png"))
        
        if not test_images:
            logger.warning("ãƒ†ã‚¹ãƒˆç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {"error": "No test images found"}
        
        # ãƒãƒƒãƒæ¨è«–å®Ÿè¡Œ
        batch_result = self.batch_inferencer.predict_directory(
            str(self.test_data_dir),
            parallel=True,
            save_results=False
        )
        
        # çµ±è¨ˆæƒ…å ±
        stats = batch_result["statistics"]
        
        accuracy_metrics = {
            "total_images": stats["total_images"],
            "successful_images": stats["successful_images"],
            "success_rate": stats["successful_images"] / stats["total_images"] * 100,
            "total_detections": stats["total_detections"],
            "avg_detections_per_image": stats["avg_detections_per_image"],
            "detection_distribution": self._analyze_detection_distribution(batch_result["results"])
        }
        
        logger.info(
            "ç²¾åº¦è©•ä¾¡å®Œäº†",
            success_rate=f"{accuracy_metrics['success_rate']:.1f}%",
            total_detections=accuracy_metrics["total_detections"]
        )
        
        return accuracy_metrics
    
    def _analyze_detection_distribution(self, results: List[Dict]) -> Dict:
        """æ¤œå‡ºæ•°åˆ†å¸ƒåˆ†æ"""
        detection_counts = [r.get("num_detections", 0) for r in results if "error" not in r]
        
        if not detection_counts:
            return {}
        
        return {
            "min_detections": min(detection_counts),
            "max_detections": max(detection_counts),
            "median_detections": np.median(detection_counts),
            "std_detections": np.std(detection_counts),
            "detection_counts": detection_counts
        }
    
    def evaluate_speed(self, num_samples: int = 100) -> Dict:
        """é€Ÿåº¦è©•ä¾¡"""
        logger.info(f"é€Ÿåº¦è©•ä¾¡é–‹å§‹ (ã‚µãƒ³ãƒ—ãƒ«æ•°: {num_samples})")
        
        # ãƒ†ã‚¹ãƒˆç”»åƒæº–å‚™
        test_images = list(self.test_data_dir.glob("*.jpg")) + \
                     list(self.test_data_dir.glob("*.png"))
        
        if not test_images:
            return {"error": "No test images found"}
        
        # ã‚µãƒ³ãƒ—ãƒ«é¸æŠ
        sample_images = (test_images * (num_samples // len(test_images) + 1))[:num_samples]
        
        # å˜ä¸€ç”»åƒæ¨è«–é€Ÿåº¦æ¸¬å®š
        single_times = []
        for image_path in sample_images[:min(50, len(sample_images))]:
            result = self.single_inferencer.predict(str(image_path))
            single_times.append(result["timing"]["total_time"])
        
        # ãƒãƒƒãƒæ¨è«–é€Ÿåº¦æ¸¬å®š
        batch_result = self.batch_inferencer.predict_batch_parallel(
            [str(img) for img in sample_images]
        )
        
        batch_stats = self.batch_inferencer.calculate_batch_statistics(
            batch_result, 
            sum(r.get("timing", {}).get("total_time", 0) for r in batch_result)
        )
        
        speed_metrics = {
            "single_inference": {
                "avg_time": np.mean(single_times),
                "min_time": np.min(single_times),
                "max_time": np.max(single_times),
                "std_time": np.std(single_times),
                "avg_fps": 1.0 / np.mean(single_times),
                "times": single_times
            },
            "batch_inference": {
                "total_time": batch_stats["total_time"],
                "avg_time": batch_stats["avg_inference_time"],
                "throughput_fps": batch_stats["throughput_fps"],
                "parallel_efficiency": batch_stats["throughput_fps"] / (1.0 / np.mean(single_times))
            },
            "num_samples": num_samples
        }
        
        logger.info(
            "é€Ÿåº¦è©•ä¾¡å®Œäº†",
            single_avg_time=f"{speed_metrics['single_inference']['avg_time']:.3f}s",
            single_avg_fps=f"{speed_metrics['single_inference']['avg_fps']:.1f} FPS",
            batch_throughput=f"{speed_metrics['batch_inference']['throughput_fps']:.1f} FPS"
        )
        
        return speed_metrics
    
    def evaluate_resource_usage(self, duration: int = 60) -> Dict:
        """ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡è©•ä¾¡"""
        logger.info(f"ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡è©•ä¾¡é–‹å§‹ (æœŸé–“: {duration}ç§’)")
        
        # ãƒ†ã‚¹ãƒˆç”»åƒæº–å‚™
        test_images = list(self.test_data_dir.glob("*.jpg")) + \
                     list(self.test_data_dir.glob("*.png"))
        
        if not test_images:
            return {"error": "No test images found"}
        
        # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ãƒ‡ãƒ¼ã‚¿
        cpu_usage = []
        memory_usage = []
        timestamps = []
        
        start_time = time.time()
        
        # ç¶™ç¶šçš„æ¨è«–ã¨ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–
        while time.time() - start_time < duration:
            # æ¨è«–å®Ÿè¡Œ
            test_image = np.random.choice(test_images)
            self.single_inferencer.predict(str(test_image))
            
            # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡è¨˜éŒ²
            cpu_percent = psutil.cpu_percent()
            memory_info = psutil.virtual_memory()
            
            cpu_usage.append(cpu_percent)
            memory_usage.append(memory_info.percent)
            timestamps.append(time.time() - start_time)
            
            time.sleep(0.1)  # 100msé–“éš”
        
        resource_metrics = {
            "duration": duration,
            "cpu_usage": {
                "avg": np.mean(cpu_usage),
                "max": np.max(cpu_usage),
                "min": np.min(cpu_usage),
                "std": np.std(cpu_usage),
                "values": cpu_usage
            },
            "memory_usage": {
                "avg": np.mean(memory_usage),
                "max": np.max(memory_usage), 
                "min": np.min(memory_usage),
                "std": np.std(memory_usage),
                "values": memory_usage
            },
            "timestamps": timestamps,
            "system_info": {
                "cpu_count": psutil.cpu_count(),
                "memory_total_gb": psutil.virtual_memory().total / (1024**3)
            }
        }
        
        logger.info(
            "ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡è©•ä¾¡å®Œäº†",
            avg_cpu=f"{resource_metrics['cpu_usage']['avg']:.1f}%",
            max_cpu=f"{resource_metrics['cpu_usage']['max']:.1f}%",
            avg_memory=f"{resource_metrics['memory_usage']['avg']:.1f}%"
        )
        
        return resource_metrics
    
    def generate_visualizations(self, evaluation_results: Dict, output_dir: str):
        """è©•ä¾¡çµæœã®å¯è¦–åŒ–"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        plt.style.use('seaborn-v0_8')
        
        # 1. æ¤œå‡ºæ•°åˆ†å¸ƒ
        if "accuracy" in evaluation_results and "detection_distribution" in evaluation_results["accuracy"]:
            detection_counts = evaluation_results["accuracy"]["detection_distribution"].get("detection_counts", [])
            if detection_counts:
                plt.figure(figsize=(10, 6))
                plt.hist(detection_counts, bins=20, alpha=0.7, color='skyblue')
                plt.xlabel('æ¤œå‡ºæ•°')
                plt.ylabel('é »åº¦')
                plt.title('æ¤œå‡ºæ•°åˆ†å¸ƒ')
                plt.grid(True, alpha=0.3)
                plt.savefig(output_dir / 'detection_distribution.png', dpi=300, bbox_inches='tight')
                plt.close()
        
        # 2. æ¨è«–æ™‚é–“åˆ†å¸ƒ
        if "speed" in evaluation_results and "single_inference" in evaluation_results["speed"]:
            times = evaluation_results["speed"]["single_inference"].get("times", [])
            if times:
                plt.figure(figsize=(12, 5))
                
                plt.subplot(1, 2, 1)
                plt.hist(times, bins=30, alpha=0.7, color='lightgreen')
                plt.xlabel('æ¨è«–æ™‚é–“ (ç§’)')
                plt.ylabel('é »åº¦')
                plt.title('æ¨è«–æ™‚é–“åˆ†å¸ƒ')
                plt.grid(True, alpha=0.3)
                
                plt.subplot(1, 2, 2)
                plt.plot(times)
                plt.xlabel('ã‚µãƒ³ãƒ—ãƒ«ç•ªå·')
                plt.ylabel('æ¨è«–æ™‚é–“ (ç§’)')
                plt.title('æ¨è«–æ™‚é–“ã®å¤‰åŒ–')
                plt.grid(True, alpha=0.3)
                
                plt.tight_layout()
                plt.savefig(output_dir / 'inference_time_analysis.png', dpi=300, bbox_inches='tight')
                plt.close()
        
        # 3. ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡
        if "resource" in evaluation_results:
            resource_data = evaluation_results["resource"]
            timestamps = resource_data.get("timestamps", [])
            cpu_values = resource_data.get("cpu_usage", {}).get("values", [])
            memory_values = resource_data.get("memory_usage", {}).get("values", [])
            
            if timestamps and cpu_values and memory_values:
                plt.figure(figsize=(12, 8))
                
                plt.subplot(2, 1, 1)
                plt.plot(timestamps, cpu_values, color='red', alpha=0.7)
                plt.fill_between(timestamps, cpu_values, alpha=0.3, color='red')
                plt.xlabel('æ™‚é–“ (ç§’)')
                plt.ylabel('CPUä½¿ç”¨ç‡ (%)')
                plt.title('CPUä½¿ç”¨ç‡ã®æ¨ç§»')
                plt.grid(True, alpha=0.3)
                
                plt.subplot(2, 1, 2)
                plt.plot(timestamps, memory_values, color='blue', alpha=0.7)
                plt.fill_between(timestamps, memory_values, alpha=0.3, color='blue')
                plt.xlabel('æ™‚é–“ (ç§’)')
                plt.ylabel('ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ (%)')
                plt.title('ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã®æ¨ç§»')
                plt.grid(True, alpha=0.3)
                
                plt.tight_layout()
                plt.savefig(output_dir / 'resource_usage.png', dpi=300, bbox_inches='tight')
                plt.close()
        
        logger.info(f"å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ç”Ÿæˆå®Œäº†: {output_dir}")
    
    def generate_html_report(
        self, 
        evaluation_results: Dict, 
        output_path: str,
        visualization_dir: Optional[str] = None
    ):
        """HTMLè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        # HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        html_template = """
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>YOLOv7n MIT Edition - è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ</title>
    <style>
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background-color: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }
        h2 { color: #34495e; margin-top: 30px; }
        .metric-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 20px 0; }
        .metric-card { background: #ecf0f1; padding: 20px; border-radius: 8px; border-left: 4px solid #3498db; }
        .metric-value { font-size: 2em; font-weight: bold; color: #2c3e50; }
        .metric-label { color: #7f8c8d; font-size: 0.9em; }
        .chart { text-align: center; margin: 20px 0; }
        .chart img { max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }
        .summary { background: #d5dbdb; padding: 20px; border-radius: 8px; margin: 20px 0; }
        .error { color: #e74c3c; background: #fadbd8; padding: 10px; border-radius: 5px; }
        table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        th, td { padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }
        th { background-color: #3498db; color: white; }
        tr:nth-child(even) { background-color: #f2f2f2; }
    </style>
</head>
<body>
    <div class="container">
        <h1>YOLOv7n MIT Edition - è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ</h1>
        
        <div class="summary">
            <h2>è©•ä¾¡ã‚µãƒãƒªãƒ¼</h2>
            <p><strong>ãƒ¢ãƒ‡ãƒ«:</strong> {{ model_name }}</p>
            <p><strong>è©•ä¾¡æ—¥æ™‚:</strong> {{ evaluation_date }}</p>
            <p><strong>ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿:</strong> {{ test_data_info }}</p>
        </div>

        {% if accuracy %}
        <h2>ğŸ¯ ç²¾åº¦è©•ä¾¡</h2>
        <div class="metric-grid">
            <div class="metric-card">
                <div class="metric-value">{{ "%.1f"|format(accuracy.success_rate) }}%</div>
                <div class="metric-label">æˆåŠŸç‡</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{{ accuracy.total_detections }}</div>
                <div class="metric-label">ç·æ¤œå‡ºæ•°</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{{ "%.2f"|format(accuracy.avg_detections_per_image) }}</div>
                <div class="metric-label">å¹³å‡æ¤œå‡ºæ•°/ç”»åƒ</div>
            </div>
        </div>
        
        {% if visualization_dir %}
        <div class="chart">
            <img src="{{ visualization_dir }}/detection_distribution.png" alt="æ¤œå‡ºæ•°åˆ†å¸ƒ">
        </div>
        {% endif %}
        {% endif %}

        {% if speed %}
        <h2>âš¡ é€Ÿåº¦è©•ä¾¡</h2>
        <div class="metric-grid">
            <div class="metric-card">
                <div class="metric-value">{{ "%.3f"|format(speed.single_inference.avg_time) }}s</div>
                <div class="metric-label">å¹³å‡æ¨è«–æ™‚é–“</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{{ "%.1f"|format(speed.single_inference.avg_fps) }}</div>
                <div class="metric-label">FPS (å˜ä¸€)</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{{ "%.1f"|format(speed.batch_inference.throughput_fps) }}</div>
                <div class="metric-label">ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (ãƒãƒƒãƒ)</div>
            </div>
        </div>
        
        {% if visualization_dir %}
        <div class="chart">
            <img src="{{ visualization_dir }}/inference_time_analysis.png" alt="æ¨è«–æ™‚é–“åˆ†æ">
        </div>
        {% endif %}
        {% endif %}

        {% if resource %}
        <h2>ğŸ’» ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡</h2>
        <div class="metric-grid">
            <div class="metric-card">
                <div class="metric-value">{{ "%.1f"|format(resource.cpu_usage.avg) }}%</div>
                <div class="metric-label">å¹³å‡CPUä½¿ç”¨ç‡</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{{ "%.1f"|format(resource.memory_usage.avg) }}%</div>
                <div class="metric-label">å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{{ "%.1f"|format(resource.system_info.memory_total_gb) }}GB</div>
                <div class="metric-label">ç·ãƒ¡ãƒ¢ãƒª</div>
            </div>
        </div>
        
        {% if visualization_dir %}
        <div class="chart">
            <img src="{{ visualization_dir }}/resource_usage.png" alt="ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡">
        </div>
        {% endif %}
        {% endif %}

        <h2>ğŸ“Š è©³ç´°ãƒ‡ãƒ¼ã‚¿</h2>
        <table>
            <tr><th>é …ç›®</th><th>å€¤</th></tr>
            {% if accuracy %}
            <tr><td>å‡¦ç†æˆåŠŸç”»åƒæ•°</td><td>{{ accuracy.successful_images }} / {{ accuracy.total_images }}</td></tr>
            {% endif %}
            {% if speed %}
            <tr><td>è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°</td><td>{{ speed.num_samples }}</td></tr>
            <tr><td>æœ€çŸ­æ¨è«–æ™‚é–“</td><td>{{ "%.3f"|format(speed.single_inference.min_time) }}s</td></tr>
            <tr><td>æœ€é•·æ¨è«–æ™‚é–“</td><td>{{ "%.3f"|format(speed.single_inference.max_time) }}s</td></tr>
            {% endif %}
            {% if resource %}
            <tr><td>CPUæœ€å¤§ä½¿ç”¨ç‡</td><td>{{ "%.1f"|format(resource.cpu_usage.max) }}%</td></tr>
            <tr><td>ãƒ¡ãƒ¢ãƒªæœ€å¤§ä½¿ç”¨ç‡</td><td>{{ "%.1f"|format(resource.memory_usage.max) }}%</td></tr>
            <tr><td>è©•ä¾¡æœŸé–“</td><td>{{ resource.duration }}ç§’</td></tr>
            {% endif %}
        </table>

        <div class="summary">
            <h2>ğŸ“ è©•ä¾¡ã¾ã¨ã‚</h2>
            <p>ã“ã®YOLOv7n MIT Editionãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã®ç‰¹å¾´ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼š</p>
            <ul>
                {% if accuracy and accuracy.success_rate > 90 %}
                <li>âœ… é«˜ã„æ¨è«–æˆåŠŸç‡ ({{ "%.1f"|format(accuracy.success_rate) }}%)</li>
                {% endif %}
                {% if speed and speed.single_inference.avg_fps > 10 %}
                <li>âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†å¯èƒ½ãªæ¨è«–é€Ÿåº¦ ({{ "%.1f"|format(speed.single_inference.avg_fps) }} FPS)</li>
                {% endif %}
                {% if resource and resource.cpu_usage.avg < 70 %}
                <li>âœ… åŠ¹ç‡çš„ãªCPUä½¿ç”¨ç‡ (å¹³å‡{{ "%.1f"|format(resource.cpu_usage.avg) }}%)</li>
                {% endif %}
                {% if speed and speed.batch_inference.parallel_efficiency > 2 %}
                <li>âœ… ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹é«˜ã„ä¸¦åˆ—åŠ¹ç‡</li>
                {% endif %}
            </ul>
        </div>
        
        <p style="text-align: center; color: #7f8c8d; margin-top: 40px;">
            Generated by YOLOv7n MIT Edition Evaluation System<br>
            {{ evaluation_date }}
        </p>
    </div>
</body>
</html>
        """
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ•°æº–å‚™
        template_vars = {
            "model_name": self.model_path.name,
            "evaluation_date": datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S"),
            "test_data_info": f"{self.test_data_dir.name} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª",
            "accuracy": evaluation_results.get("accuracy"),
            "speed": evaluation_results.get("speed"),
            "resource": evaluation_results.get("resource"),
            "visualization_dir": visualization_dir
        }
        
        # HTMLãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
        template = Template(html_template)
        html_content = template.render(**template_vars)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        output_path = Path(output_path)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"HTMLè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {output_path}")
    
    def run_full_evaluation(self, output_dir: str) -> Dict:
        """å®Œå…¨è©•ä¾¡å®Ÿè¡Œ"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("å®Œå…¨è©•ä¾¡é–‹å§‹")
        
        # å„ç¨®è©•ä¾¡å®Ÿè¡Œ
        evaluation_results = {}
        
        try:
            evaluation_results["accuracy"] = self.evaluate_accuracy()
        except Exception as e:
            logger.error(f"ç²¾åº¦è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            evaluation_results["accuracy"] = {"error": str(e)}
        
        try:
            evaluation_results["speed"] = self.evaluate_speed()
        except Exception as e:
            logger.error(f"é€Ÿåº¦è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            evaluation_results["speed"] = {"error": str(e)}
        
        try:
            evaluation_results["resource"] = self.evaluate_resource_usage()
        except Exception as e:
            logger.error(f"ãƒªã‚½ãƒ¼ã‚¹è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            evaluation_results["resource"] = {"error": str(e)}
        
        # çµæœä¿å­˜
        results_file = output_dir / "evaluation_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
        
        # å¯è¦–åŒ–ç”Ÿæˆ
        viz_dir = output_dir / "visualizations"
        self.generate_visualizations(evaluation_results, viz_dir)
        
        # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        html_report = output_dir / "evaluation_report.html"
        self.generate_html_report(
            evaluation_results, 
            html_report,
            visualization_dir="visualizations"
        )
        
        logger.info(
            "å®Œå…¨è©•ä¾¡å®Œäº†",
            output_dir=str(output_dir),
            results_file=str(results_file),
            html_report=str(html_report)
        )
        
        return evaluation_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="YOLOv7nè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
    parser.add_argument("--model", required=True, help="è©•ä¾¡ã™ã‚‹ONNXãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--test-data", required=True, help="ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--output", default="analysis/evaluation", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--speed-samples", type=int, default=100, help="é€Ÿåº¦è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°")
    parser.add_argument("--resource-duration", type=int, default=60, help="ãƒªã‚½ãƒ¼ã‚¹è©•ä¾¡æœŸé–“(ç§’)")
    
    args = parser.parse_args()
    
    # è©•ä¾¡å®Ÿè¡Œ
    evaluator = ModelEvaluator(args.model, args.test_data)
    results = evaluator.run_full_evaluation(args.output)
    
    # çµæœè¡¨ç¤º
    print(f"\n=== è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼ ===")
    
    if "accuracy" in results and "error" not in results["accuracy"]:
        acc = results["accuracy"]
        print(f"ç²¾åº¦è©•ä¾¡:")
        print(f"  æˆåŠŸç‡: {acc['success_rate']:.1f}%")
        print(f"  ç·æ¤œå‡ºæ•°: {acc['total_detections']}")
    
    if "speed" in results and "error" not in results["speed"]:
        speed = results["speed"]
        print(f"é€Ÿåº¦è©•ä¾¡:")
        print(f"  å¹³å‡æ¨è«–æ™‚é–“: {speed['single_inference']['avg_time']:.3f}s")
        print(f"  å¹³å‡FPS: {speed['single_inference']['avg_fps']:.1f}")
    
    if "resource" in results and "error" not in results["resource"]:
        resource = results["resource"]
        print(f"ãƒªã‚½ãƒ¼ã‚¹è©•ä¾¡:")
        print(f"  å¹³å‡CPUä½¿ç”¨ç‡: {resource['cpu_usage']['avg']:.1f}%")
        print(f"  å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {resource['memory_usage']['avg']:.1f}%")
    
    print(f"\nè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {args.output}/evaluation_report.html")

if __name__ == "__main__":
    main()