"""
YOLOv7n MIT Edition - åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆ
Dockerç’°å¢ƒã§ã®å®Œå…¨ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
"""

import os
import time
from pathlib import Path
from typing import Dict, List

from evaluation_report import ModelEvaluator
from inference.logger import get_logger
from quantize_onnx import ONNXQuantizer

logger = get_logger("benchmark")

class BenchmarkSuite:
    """åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆ"""
    
    def __init__(self, models_dir: str = "models", data_dir: str = "data"):
        """
        ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆåˆæœŸåŒ–
        
        Args:
            models_dir: ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.models_dir = Path(models_dir)
        self.data_dir = Path(data_dir)
        self.results = {}
        
        logger.info(
            "ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆåˆæœŸåŒ–",
            models_dir=str(self.models_dir),
            data_dir=str(self.data_dir)
        )
    
    def discover_models(self) -> List[Path]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ç™ºè¦‹"""
        model_files = []
        
        # ONNXãƒ¢ãƒ‡ãƒ«æ¤œç´¢
        for pattern in ["*.onnx", "**/*.onnx"]:
            model_files.extend(self.models_dir.glob(pattern))
        
        logger.info(f"ç™ºè¦‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æ•°: {len(model_files)}")
        for model in model_files:
            logger.info(f"  - {model.name}")
        
        return model_files
    
    def discover_test_data(self) -> List[Path]:
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç™ºè¦‹"""
        data_dirs = []
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¤œç´¢
        for potential_dir in self.data_dir.iterdir():
            if potential_dir.is_dir():
                # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                image_files = []
                for ext in [".jpg", ".jpeg", ".png", ".bmp"]:
                    image_files.extend(potential_dir.glob(f"*{ext}"))
                    image_files.extend(potential_dir.glob(f"*{ext.upper()}"))
                
                if image_files:
                    data_dirs.append(potential_dir)
        
        logger.info(f"ç™ºè¦‹ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {len(data_dirs)}")
        for data_dir in data_dirs:
            logger.info(f"  - {data_dir.name}")
        
        return data_dirs
    
    def benchmark_model(self, model_path: Path, test_data_dir: Path) -> Dict:
        """å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        logger.info(
            "ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹",
            model=model_path.name,
            test_data=test_data_dir.name
        )
        
        try:
            # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡å®Ÿè¡Œ
            evaluator = ModelEvaluator(str(model_path), str(test_data_dir))
            
            # è»½é‡ç‰ˆè©•ä¾¡ï¼ˆDockerç’°å¢ƒã§ã®é«˜é€Ÿå®Ÿè¡Œï¼‰
            results = {}
            
            # ç²¾åº¦è©•ä¾¡
            results["accuracy"] = evaluator.evaluate_accuracy()
            
            # é€Ÿåº¦è©•ä¾¡ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°å‰Šæ¸›ï¼‰
            results["speed"] = evaluator.evaluate_speed(num_samples=20)
            
            # ãƒªã‚½ãƒ¼ã‚¹è©•ä¾¡ï¼ˆæœŸé–“çŸ­ç¸®ï¼‰
            results["resource"] = evaluator.evaluate_resource_usage(duration=30)
            
            # åŸºæœ¬æƒ…å ±è¿½åŠ 
            results["model_info"] = {
                "name": model_path.name,
                "size_mb": model_path.stat().st_size / (1024 * 1024),
                "test_dataset": test_data_dir.name
            }
            
            logger.info(
                "ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†",
                model=model_path.name,
                success_rate=results.get("accuracy", {}).get("success_rate", 0),
                avg_fps=results.get("speed", {}).get("single_inference", {}).get("avg_fps", 0)
            )
            
            return results
            
        except Exception as e:
            logger.error(
                "ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼",
                model=model_path.name,
                error=str(e)
            )
            return {
                "error": str(e),
                "model_info": {
                    "name": model_path.name,
                    "size_mb": model_path.stat().st_size / (1024 * 1024),
                    "test_dataset": test_data_dir.name
                }
            }
    
    def run_quantization_benchmark(self, model_path: Path) -> Dict:
        """é‡å­åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        logger.info(f"é‡å­åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹: {model_path.name}")
        
        try:
            quantizer = ONNXQuantizer(str(model_path))
            
            # ä¸€æ™‚å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            temp_output = Path("temp_quantized")
            temp_output.mkdir(exist_ok=True)
            
            # å‹•çš„é‡å­åŒ–ã®ã¿å®Ÿè¡Œï¼ˆé«˜é€ŸåŒ–ï¼‰
            dynamic_output = temp_output / f"{model_path.stem}_dynamic.onnx"
            
            result = quantizer.quantize_dynamic(str(dynamic_output))
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            if dynamic_output.exists():
                dynamic_output.unlink()
            
            logger.info(
                "é‡å­åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†",
                model=model_path.name,
                compression_ratio=result.get("compression_ratio", 0)
            )
            
            return result
            
        except Exception as e:
            logger.error(
                "é‡å­åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼",
                model=model_path.name,
                error=str(e)
            )
            return {"error": str(e)}
    
    def run_full_benchmark(self) -> Dict:
        """å®Œå…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        logger.info("å®Œå…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
        start_time = time.time()
        
        # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ç™ºè¦‹
        models = self.discover_models()
        test_datasets = self.discover_test_data()
        
        if not models:
            logger.error("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¯¾è±¡ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {"error": "No models found"}
        
        if not test_datasets:
            logger.error("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {"error": "No test data found"}
        
        benchmark_results = {
            "summary": {
                "start_time": time.time(),
                "models_count": len(models),
                "datasets_count": len(test_datasets)
            },
            "model_benchmarks": {},
            "quantization_benchmarks": {},
            "comparison": {}
        }
        
        # å„ãƒ¢ãƒ‡ãƒ«ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        for model_path in models:
            model_name = model_path.name
            benchmark_results["model_benchmarks"][model_name] = {}
            
            # å„ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ†ã‚¹ãƒˆ
            for test_data_dir in test_datasets:
                dataset_name = test_data_dir.name
                
                # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
                result = self.benchmark_model(model_path, test_data_dir)
                benchmark_results["model_benchmarks"][model_name][dataset_name] = result
            
            # é‡å­åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            quant_result = self.run_quantization_benchmark(model_path)
            benchmark_results["quantization_benchmarks"][model_name] = quant_result
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒåˆ†æ
        benchmark_results["comparison"] = self.analyze_benchmarks(
            benchmark_results["model_benchmarks"]
        )
        
        # å®Œäº†æƒ…å ±
        total_time = time.time() - start_time
        benchmark_results["summary"]["total_time"] = total_time
        benchmark_results["summary"]["end_time"] = time.time()
        
        logger.info(
            "å®Œå…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†",
            total_time=f"{total_time:.2f}s",
            models_tested=len(models),
            datasets_used=len(test_datasets)
        )
        
        return benchmark_results
    
    def analyze_benchmarks(self, model_benchmarks: Dict) -> Dict:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœåˆ†æ"""
        analysis = {
            "best_accuracy": {"model": "", "dataset": "", "success_rate": 0},
            "best_speed": {"model": "", "dataset": "", "fps": 0},
            "best_efficiency": {"model": "", "dataset": "", "efficiency_score": 0},
            "model_rankings": []
        }
        
        model_scores = {}
        
        for model_name, datasets in model_benchmarks.items():
            model_scores[model_name] = {
                "avg_success_rate": 0,
                "avg_fps": 0,
                "avg_cpu_usage": 0,
                "avg_memory_usage": 0,
                "efficiency_score": 0
            }
            
            success_rates = []
            fps_values = []
            cpu_values = []
            memory_values = []
            
            for dataset_name, result in datasets.items():
                if "error" in result:
                    continue
                
                # ç²¾åº¦ãƒ‡ãƒ¼ã‚¿
                if "accuracy" in result and "success_rate" in result["accuracy"]:
                    success_rate = result["accuracy"]["success_rate"]
                    success_rates.append(success_rate)
                    
                    if success_rate > analysis["best_accuracy"]["success_rate"]:
                        analysis["best_accuracy"] = {
                            "model": model_name,
                            "dataset": dataset_name,
                            "success_rate": success_rate
                        }
                
                # é€Ÿåº¦ãƒ‡ãƒ¼ã‚¿
                if "speed" in result and "single_inference" in result["speed"]:
                    fps = result["speed"]["single_inference"].get("avg_fps", 0)
                    fps_values.append(fps)
                    
                    if fps > analysis["best_speed"]["fps"]:
                        analysis["best_speed"] = {
                            "model": model_name,
                            "dataset": dataset_name,
                            "fps": fps
                        }
                
                # ãƒªã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
                if "resource" in result:
                    cpu_usage = result["resource"].get("cpu_usage", {}).get("avg", 0)
                    memory_usage = result["resource"].get("memory_usage", {}).get("avg", 0)
                    cpu_values.append(cpu_usage)
                    memory_values.append(memory_usage)
            
            # å¹³å‡å€¤è¨ˆç®—
            if success_rates:
                model_scores[model_name]["avg_success_rate"] = sum(success_rates) / len(success_rates)
            if fps_values:
                model_scores[model_name]["avg_fps"] = sum(fps_values) / len(fps_values)
            if cpu_values:
                model_scores[model_name]["avg_cpu_usage"] = sum(cpu_values) / len(cpu_values)
            if memory_values:
                model_scores[model_name]["avg_memory_usage"] = sum(memory_values) / len(memory_values)
            
            # åŠ¹ç‡ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç²¾åº¦ Ã— é€Ÿåº¦ / ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ï¼‰
            if (model_scores[model_name]["avg_success_rate"] > 0 and
                model_scores[model_name]["avg_fps"] > 0 and
                model_scores[model_name]["avg_cpu_usage"] > 0):
                
                efficiency = (
                    model_scores[model_name]["avg_success_rate"] * 
                    model_scores[model_name]["avg_fps"]
                ) / (model_scores[model_name]["avg_cpu_usage"] / 100)
                
                model_scores[model_name]["efficiency_score"] = efficiency
                
                if efficiency > analysis["best_efficiency"]["efficiency_score"]:
                    analysis["best_efficiency"] = {
                        "model": model_name,
                        "dataset": "average",
                        "efficiency_score": efficiency
                    }
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½œæˆ
        analysis["model_rankings"] = sorted(
            [(name, scores) for name, scores in model_scores.items()],
            key=lambda x: x[1]["efficiency_score"],
            reverse=True
        )
        
        return analysis
    
    def save_benchmark_results(self, results: Dict, output_path: str):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœä¿å­˜"""
        import json
        
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœä¿å­˜: {output_path}")
    
    def print_benchmark_summary(self, results: Dict):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("\n" + "="*60)
        print("ğŸš€ YOLOv7n MIT Edition - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ")
        print("="*60)
        
        summary = results.get("summary", {})
        print(f"ğŸ“Š å®Ÿè¡Œã‚µãƒãƒªãƒ¼:")
        print(f"  ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«æ•°: {summary.get('models_count', 0)}")
        print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {summary.get('datasets_count', 0)}")
        print(f"  ç·å®Ÿè¡Œæ™‚é–“: {summary.get('total_time', 0):.2f}ç§’")
        
        comparison = results.get("comparison", {})
        
        if "best_accuracy" in comparison:
            best_acc = comparison["best_accuracy"]
            print(f"\nğŸ¯ æœ€é«˜ç²¾åº¦:")
            print(f"  ãƒ¢ãƒ‡ãƒ«: {best_acc.get('model', 'N/A')}")
            print(f"  æˆåŠŸç‡: {best_acc.get('success_rate', 0):.1f}%")
        
        if "best_speed" in comparison:
            best_speed = comparison["best_speed"]
            print(f"\nâš¡ æœ€é«˜é€Ÿåº¦:")
            print(f"  ãƒ¢ãƒ‡ãƒ«: {best_speed.get('model', 'N/A')}")
            print(f"  FPS: {best_speed.get('fps', 0):.1f}")
        
        if "best_efficiency" in comparison:
            best_eff = comparison["best_efficiency"]
            print(f"\nğŸ† æœ€é«˜åŠ¹ç‡:")
            print(f"  ãƒ¢ãƒ‡ãƒ«: {best_eff.get('model', 'N/A')}")
            print(f"  åŠ¹ç‡ã‚¹ã‚³ã‚¢: {best_eff.get('efficiency_score', 0):.2f}")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        rankings = comparison.get("model_rankings", [])
        if rankings:
            print(f"\nğŸ“ˆ ãƒ¢ãƒ‡ãƒ«åŠ¹ç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
            for i, (model_name, scores) in enumerate(rankings[:3], 1):
                print(f"  {i}. {model_name}")
                print(f"     ç²¾åº¦: {scores['avg_success_rate']:.1f}%")
                print(f"     é€Ÿåº¦: {scores['avg_fps']:.1f} FPS")
                print(f"     åŠ¹ç‡: {scores['efficiency_score']:.2f}")
        
        print("\n" + "="*60)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="YOLOv7nåŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    parser.add_argument("--models-dir", default="models", help="ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--data-dir", default="data", help="ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--output", default="analysis/benchmark_results.json", help="çµæœå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«")
    
    args = parser.parse_args()
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    benchmark_suite = BenchmarkSuite(args.models_dir, args.data_dir)
    results = benchmark_suite.run_full_benchmark()
    
    # çµæœä¿å­˜
    benchmark_suite.save_benchmark_results(results, args.output)
    
    # çµæœè¡¨ç¤º
    benchmark_suite.print_benchmark_summary(results)
    
    print(f"\nè©³ç´°çµæœ: {args.output}")

if __name__ == "__main__":
    main()