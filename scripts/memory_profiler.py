#!/usr/bin/env python3
"""
ONNX ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«
æ¨è«–ä¸­ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©³ç´°åˆ†æã—æœ€é©åŒ–æ¡ˆæç¤º
"""
import time
import numpy as np
import onnxruntime as ort
from pathlib import Path
import sys
import psutil
import gc
import tracemalloc
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import threading

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class MemoryProfiler:
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼"""
    
    def __init__(self):
        self.memory_samples = []
        self.time_samples = []
        self.monitoring = False
        self.monitor_thread = None
    
    def start_monitoring(self, interval: float = 0.001):
        """ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹"""
        self.memory_samples = []
        self.time_samples = []
        self.monitoring = True
        self.start_time = time.time()
        
        def monitor():
            while self.monitoring:
                current_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
                current_time = time.time() - self.start_time
                
                self.memory_samples.append(current_memory)
                self.time_samples.append(current_time)
                
                time.sleep(interval)
        
        self.monitor_thread = threading.Thread(target=monitor)
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """ãƒ¡ãƒ¢ãƒªç›£è¦–åœæ­¢"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
    
    def get_stats(self):
        """ãƒ¡ãƒ¢ãƒªçµ±è¨ˆå–å¾—"""
        if not self.memory_samples:
            return None
        
        memory_array = np.array(self.memory_samples)
        
        return {
            'max_memory_mb': np.max(memory_array),
            'min_memory_mb': np.min(memory_array),
            'avg_memory_mb': np.mean(memory_array),
            'peak_increase_mb': np.max(memory_array) - np.min(memory_array),
            'samples_count': len(memory_array),
            'duration_seconds': self.time_samples[-1] if self.time_samples else 0
        }

def profile_model_loading():
    """ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ™‚ã®ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°"""
    print("=== ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚° ===")
    
    onnx_path = project_root / "exports/onnx/yolov7n_mezzopiano.onnx"
    
    # ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹
    profiler = MemoryProfiler()
    profiler.start_monitoring()
    
    # åˆæœŸãƒ¡ãƒ¢ãƒª
    initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
    print(f"ğŸ” åˆæœŸãƒ¡ãƒ¢ãƒª: {initial_memory:.1f}MB")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    print("ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
    load_start = time.time()
    
    session = ort.InferenceSession(str(onnx_path))
    
    load_end = time.time()
    
    # ãƒ­ãƒ¼ãƒ‰å¾Œãƒ¡ãƒ¢ãƒª
    post_load_memory = psutil.Process().memory_info().rss / 1024 / 1024
    print(f"ğŸ“ ãƒ­ãƒ¼ãƒ‰å®Œäº†: {load_end - load_start:.2f}ç§’")
    print(f"ğŸ“Š ãƒ­ãƒ¼ãƒ‰å¾Œãƒ¡ãƒ¢ãƒª: {post_load_memory:.1f}MB")
    print(f"ğŸ“ˆ ãƒ¡ãƒ¢ãƒªå¢—åŠ : {post_load_memory - initial_memory:.1f}MB")
    
    profiler.stop_monitoring()
    
    return {
        'session': session,
        'load_time': load_end - load_start,
        'initial_memory': initial_memory,
        'post_load_memory': post_load_memory,
        'memory_increase': post_load_memory - initial_memory,
        'profiler_stats': profiler.get_stats()
    }

def profile_inference_memory(session, num_inferences: int = 100):
    """æ¨è«–æ™‚ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°"""
    print(f"\n=== æ¨è«–æ™‚ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚° ({num_inferences}å›) ===")
    
    input_name = session.get_inputs()[0].name
    dummy_input = np.random.randn(1, 3, 640, 640).astype(np.float32)
    
    # ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹
    profiler = MemoryProfiler()
    profiler.start_monitoring()
    
    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    print("ğŸ”¥ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œ...")
    for _ in range(5):
        _ = session.run(None, {input_name: dummy_input})
    
    # æ¨è«–å®Ÿè¡Œ
    print(f"âš¡ æ¨è«–å®Ÿè¡Œä¸­ ({num_inferences}å›)...")
    inference_start = time.time()
    
    for i in range(num_inferences):
        outputs = session.run(None, {input_name: dummy_input})
        
        if (i + 1) % 20 == 0:
            current_memory = psutil.Process().memory_info().rss / 1024 / 1024
            print(f"   {i+1:3d}å›ç›®: {current_memory:.1f}MB")
    
    inference_end = time.time()
    
    profiler.stop_monitoring()
    
    stats = profiler.get_stats()
    total_time = inference_end - inference_start
    avg_inference_time = total_time / num_inferences * 1000  # ms
    
    print(f"âœ… æ¨è«–å®Œäº†: {total_time:.2f}ç§’")
    print(f"ğŸ“Š å¹³å‡æ¨è«–æ™‚é–“: {avg_inference_time:.1f}ms")
    print(f"ğŸ“ˆ æœ€å¤§ãƒ¡ãƒ¢ãƒª: {stats['max_memory_mb']:.1f}MB")
    print(f"ğŸ“‰ æœ€å°ãƒ¡ãƒ¢ãƒª: {stats['min_memory_mb']:.1f}MB")
    print(f"ğŸ“Š ãƒ¡ãƒ¢ãƒªå¤‰å‹•: {stats['peak_increase_mb']:.1f}MB")
    
    return {
        'inference_stats': stats,
        'total_time': total_time,
        'avg_inference_time': avg_inference_time,
        'memory_samples': profiler.memory_samples,
        'time_samples': profiler.time_samples
    }

def profile_batch_inference(session, batch_sizes: List[int] = [1, 2, 4, 8]):
    """ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¥ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°"""
    print(f"\n=== ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¥ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚° ===")
    
    input_name = session.get_inputs()[0].name
    results = {}
    
    for batch_size in batch_sizes:
        print(f"\nğŸ” ãƒãƒƒãƒã‚µã‚¤ã‚º {batch_size} ãƒ†ã‚¹ãƒˆ")
        
        # ãƒãƒƒãƒå…¥åŠ›ä½œæˆ
        batch_input = np.random.randn(batch_size, 3, 640, 640).astype(np.float32)
        
        # ãƒ¡ãƒ¢ãƒªç›£è¦–
        profiler = MemoryProfiler()
        profiler.start_monitoring()
        
        # æ¨è«–å®Ÿè¡Œ
        try:
            start_time = time.time()
            outputs = session.run(None, {input_name: batch_input})
            end_time = time.time()
            
            profiler.stop_monitoring()
            
            inference_time = (end_time - start_time) * 1000  # ms
            per_sample_time = inference_time / batch_size
            
            stats = profiler.get_stats()
            
            results[batch_size] = {
                'success': True,
                'total_time_ms': inference_time,
                'per_sample_time_ms': per_sample_time,
                'memory_stats': stats,
                'output_shapes': [out.shape for out in outputs]
            }
            
            print(f"âœ… æˆåŠŸ: {inference_time:.1f}ms ({per_sample_time:.1f}ms/sample)")
            print(f"ğŸ“Š æœ€å¤§ãƒ¡ãƒ¢ãƒª: {stats['max_memory_mb']:.1f}MB")
            
        except Exception as e:
            profiler.stop_monitoring()
            results[batch_size] = {
                'success': False,
                'error': str(e)
            }
            print(f"âŒ å¤±æ•—: {e}")
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        gc.collect()
        time.sleep(1)
    
    return results

def analyze_memory_optimization():
    """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–åˆ†æ"""
    print(f"\n=== ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–åˆ†æ ===")
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—
    system_memory = psutil.virtual_memory()
    process = psutil.Process()
    
    analysis = {
        'system_memory_total_gb': system_memory.total / 1024**3,
        'system_memory_available_gb': system_memory.available / 1024**3,
        'system_memory_used_percent': system_memory.percent,
        'process_memory_mb': process.memory_info().rss / 1024 / 1024,
        'process_memory_percent': process.memory_percent()
    }
    
    print(f"ğŸ–¥ï¸ ã‚·ã‚¹ãƒ†ãƒ ç·ãƒ¡ãƒ¢ãƒª: {analysis['system_memory_total_gb']:.1f}GB")
    print(f"ğŸ’¾ åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {analysis['system_memory_available_gb']:.1f}GB")
    print(f"ğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {analysis['system_memory_used_percent']:.1f}%")
    print(f"ğŸ”§ ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒª: {analysis['process_memory_mb']:.1f}MB")
    
    # æœ€é©åŒ–æ¨å¥¨äº‹é …
    recommendations = []
    
    if analysis['process_memory_mb'] > 500:
        recommendations.append("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ500MBè¶… - ãƒ¢ãƒ‡ãƒ«é‡å­åŒ–ã‚’æ¤œè¨")
    
    if analysis['system_memory_used_percent'] > 80:
        recommendations.append("ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒ80%è¶… - ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãŒå¿…è¦")
    
    if analysis['system_memory_available_gb'] < 2:
        recommendations.append("åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªãŒ2GBæœªæº€ - è»½é‡åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨")
    
    analysis['recommendations'] = recommendations
    
    return analysis

def create_memory_visualization(inference_result: Dict, output_dir: Path):
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¯è¦–åŒ–"""
    print(f"\n=== ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¯è¦–åŒ–ä½œæˆ ===")
    
    try:
        plt.figure(figsize=(12, 8))
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¨ç§»
        plt.subplot(2, 2, 1)
        plt.plot(inference_result['time_samples'], inference_result['memory_samples'], 'b-', linewidth=1)
        plt.title('æ¨è«–ä¸­ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¨ç§»')
        plt.xlabel('æ™‚é–“ (ç§’)')
        plt.ylabel('ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)')
        plt.grid(True)
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡åˆ†å¸ƒ
        plt.subplot(2, 2, 2)
        plt.hist(inference_result['memory_samples'], bins=30, alpha=0.7, color='green')
        plt.title('ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡åˆ†å¸ƒ')
        plt.xlabel('ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)')
        plt.ylabel('é »åº¦')
        plt.grid(True)
        
        # çµ±è¨ˆæƒ…å ±
        plt.subplot(2, 2, 3)
        stats = inference_result['inference_stats']
        labels = ['æœ€å¤§', 'å¹³å‡', 'æœ€å°']
        values = [stats['max_memory_mb'], stats['avg_memory_mb'], stats['min_memory_mb']]
        plt.bar(labels, values, color=['red', 'blue', 'green'])
        plt.title('ãƒ¡ãƒ¢ãƒªçµ±è¨ˆ')
        plt.ylabel('ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)')
        plt.grid(True)
        
        # æ¨è«–æ™‚é–“çµ±è¨ˆ
        plt.subplot(2, 2, 4)
        plt.text(0.1, 0.8, f"å¹³å‡æ¨è«–æ™‚é–“: {inference_result['avg_inference_time']:.1f}ms", fontsize=12)
        plt.text(0.1, 0.6, f"ç·å®Ÿè¡Œæ™‚é–“: {inference_result['total_time']:.2f}ç§’", fontsize=12)
        plt.text(0.1, 0.4, f"ãƒ¡ãƒ¢ãƒªå¤‰å‹•: {stats['peak_increase_mb']:.1f}MB", fontsize=12)
        plt.text(0.1, 0.2, f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {stats['samples_count']}", fontsize=12)
        plt.title('å®Ÿè¡Œçµ±è¨ˆ')
        plt.axis('off')
        
        plt.tight_layout()
        
        # ä¿å­˜
        viz_path = output_dir / 'memory_profile_visualization.png'
        plt.savefig(viz_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… å¯è¦–åŒ–ä¿å­˜: {viz_path}")
        return viz_path
        
    except Exception as e:
        print(f"âš ï¸ å¯è¦–åŒ–ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None

def generate_memory_report(load_result: Dict, inference_result: Dict, batch_results: Dict, optimization_analysis: Dict, output_dir: Path):
    """ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    print(f"\n=== ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ ===")
    
    report_path = output_dir / "memory_profile_report.md"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# YOLOv7n ONNX ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"**ç”Ÿæˆæ—¥æ™‚**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        f.write("## ğŸ–¥ï¸ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±\n\n")
        f.write(f"- **ç·ãƒ¡ãƒ¢ãƒª**: {optimization_analysis['system_memory_total_gb']:.1f}GB\n")
        f.write(f"- **åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª**: {optimization_analysis['system_memory_available_gb']:.1f}GB\n")
        f.write(f"- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡**: {optimization_analysis['system_memory_used_percent']:.1f}%\n")
        f.write(f"- **ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒª**: {optimization_analysis['process_memory_mb']:.1f}MB\n\n")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        f.write("## ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰åˆ†æ\n\n")
        f.write(f"- **ãƒ­ãƒ¼ãƒ‰æ™‚é–“**: {load_result['load_time']:.2f}ç§’\n")
        f.write(f"- **åˆæœŸãƒ¡ãƒ¢ãƒª**: {load_result['initial_memory']:.1f}MB\n")
        f.write(f"- **ãƒ­ãƒ¼ãƒ‰å¾Œãƒ¡ãƒ¢ãƒª**: {load_result['post_load_memory']:.1f}MB\n")
        f.write(f"- **ãƒ¡ãƒ¢ãƒªå¢—åŠ **: {load_result['memory_increase']:.1f}MB\n\n")
        
        # æ¨è«–ãƒ¡ãƒ¢ãƒª
        f.write("## âš¡ æ¨è«–ãƒ¡ãƒ¢ãƒªåˆ†æ\n\n")
        stats = inference_result['inference_stats']
        f.write(f"- **å¹³å‡æ¨è«–æ™‚é–“**: {inference_result['avg_inference_time']:.1f}ms\n")
        f.write(f"- **æœ€å¤§ãƒ¡ãƒ¢ãƒª**: {stats['max_memory_mb']:.1f}MB\n")
        f.write(f"- **æœ€å°ãƒ¡ãƒ¢ãƒª**: {stats['min_memory_mb']:.1f}MB\n")
        f.write(f"- **å¹³å‡ãƒ¡ãƒ¢ãƒª**: {stats['avg_memory_mb']:.1f}MB\n")
        f.write(f"- **ãƒ¡ãƒ¢ãƒªå¤‰å‹•**: {stats['peak_increase_mb']:.1f}MB\n\n")
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ†æ
        f.write("## ğŸ“Š ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¥åˆ†æ\n\n")
        f.write("| ãƒãƒƒãƒã‚µã‚¤ã‚º | æ¨è«–æ™‚é–“(ms) | ã‚µãƒ³ãƒ—ãƒ«å˜ä¾¡(ms) | æœ€å¤§ãƒ¡ãƒ¢ãƒª(MB) | ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ |\n")
        f.write("|-------------|-------------|-----------------|-------------|----------|\n")
        
        for batch_size, result in batch_results.items():
            if result['success']:
                f.write(f"| {batch_size} | {result['total_time_ms']:.1f} | {result['per_sample_time_ms']:.1f} | {result['memory_stats']['max_memory_mb']:.1f} | âœ… æˆåŠŸ |\n")
            else:
                f.write(f"| {batch_size} | - | - | - | âŒ å¤±æ•— |\n")
        
        # æœ€é©åŒ–æ¨å¥¨
        f.write("\n## ğŸ¯ æœ€é©åŒ–æ¨å¥¨äº‹é …\n\n")
        if optimization_analysis['recommendations']:
            for rec in optimization_analysis['recommendations']:
                f.write(f"- âš ï¸ {rec}\n")
        else:
            f.write("- âœ… ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯é©åˆ‡ãªç¯„å›²å†…ã§ã™\n")
        
        f.write("\n### ä¸€èˆ¬çš„ãªæœ€é©åŒ–æ‰‹æ³•\n")
        f.write("1. **ãƒ¢ãƒ‡ãƒ«é‡å­åŒ–**: FP32 â†’ FP16 ã¾ãŸã¯ INT8 å¤‰æ›\n")
        f.write("2. **ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´**: ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ã«åˆã‚ã›ãŸæœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚º\n")
        f.write("3. **ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æœ€é©åŒ–**: CUDAã€TensorRTç­‰ã®åˆ©ç”¨\n")
        f.write("4. **ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«è¨­å®š**: ONNX Runtimeãƒ¡ãƒ¢ãƒªè¨­å®šèª¿æ•´\n\n")
    
    print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
    return report_path

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸ” ONNX ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°é–‹å§‹")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = project_root / "analysis/memory_profiling"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        # 1. ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
        load_result = profile_model_loading()
        
        # 2. æ¨è«–ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
        inference_result = profile_inference_memory(load_result['session'], num_inferences=100)
        
        # 3. ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¥ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
        batch_results = profile_batch_inference(load_result['session'])
        
        # 4. ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–åˆ†æ
        optimization_analysis = analyze_memory_optimization()
        
        # 5. å¯è¦–åŒ–ä½œæˆ
        viz_path = create_memory_visualization(inference_result, output_dir)
        
        # 6. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_path = generate_memory_report(
            load_result, inference_result, batch_results, 
            optimization_analysis, output_dir
        )
        
        print(f"\nğŸ‰ ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Œäº†!")
        print(f"ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
        if viz_path:
            print(f"ğŸ“Š å¯è¦–åŒ–: {viz_path}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)