#!/usr/bin/env python3
"""
ONNX Runtime æ¨è«–é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ»æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
YOLOv7n ONNX ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ã¨æœ€é©åŒ–è¨­å®šãƒ†ã‚¹ãƒˆ
"""
import time
import numpy as np
import onnxruntime as ort
from pathlib import Path
import sys
import psutil
import gc
from typing import Dict, List, Tuple

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def get_available_providers():
    """åˆ©ç”¨å¯èƒ½ãªONNX Runtime ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å–å¾—"""
    print("=== åˆ©ç”¨å¯èƒ½ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ ===")
    
    available = ort.get_available_providers()
    print(f"åˆ©ç”¨å¯èƒ½: {available}")
    
    # æ¨å¥¨é †åºã§æ•´ç†
    recommended_order = []
    
    # GPUå„ªå…ˆ (åˆ©ç”¨å¯èƒ½ãªå ´åˆ)
    if 'CUDAExecutionProvider' in available:
        recommended_order.append('CUDAExecutionProvider')
        print("âœ… CUDA GPU ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ©ç”¨å¯èƒ½")
    
    if 'TensorrtExecutionProvider' in available:
        recommended_order.append('TensorrtExecutionProvider')
        print("âœ… TensorRT ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ©ç”¨å¯èƒ½")
    
    # CPUæœ€é©åŒ–
    if 'CPUExecutionProvider' in available:
        recommended_order.append('CPUExecutionProvider')
        print("âœ… CPUå®Ÿè¡Œåˆ©ç”¨å¯èƒ½")
    
    return recommended_order

def create_session_configs():
    """å„ç¨®ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šã‚’ä½œæˆ"""
    print("\n=== ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šä½œæˆ ===")
    
    configs = {}
    
    # 1. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    configs['default'] = {
        'providers': ['CPUExecutionProvider'],
        'session_options': None,
        'description': 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆCPUå®Ÿè¡Œ'
    }
    
    # 2. CPUæœ€é©åŒ–è¨­å®š
    cpu_options = ort.SessionOptions()
    cpu_options.intra_op_num_threads = psutil.cpu_count()
    cpu_options.inter_op_num_threads = 1
    cpu_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
    cpu_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
    
    configs['cpu_optimized'] = {
        'providers': ['CPUExecutionProvider'],
        'session_options': cpu_options,
        'description': f'CPUæœ€é©åŒ– ({psutil.cpu_count()}ã‚¹ãƒ¬ãƒƒãƒ‰)'
    }
    
    # 3. GPUè¨­å®š (åˆ©ç”¨å¯èƒ½ãªå ´åˆ)
    available = get_available_providers()
    if 'CUDAExecutionProvider' in available:
        gpu_options = ort.SessionOptions()
        gpu_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        
        configs['gpu_cuda'] = {
            'providers': ['CUDAExecutionProvider', 'CPUExecutionProvider'],
            'session_options': gpu_options,
            'description': 'CUDA GPU ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³'
        }
    
    # 4. ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š
    memory_options = ort.SessionOptions()
    memory_options.enable_mem_pattern = True
    memory_options.enable_cpu_mem_arena = True
    memory_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
    
    configs['memory_optimized'] = {
        'providers': ['CPUExecutionProvider'],
        'session_options': memory_options,
        'description': 'ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–'
    }
    
    for name, config in configs.items():
        print(f"âœ… {name}: {config['description']}")
    
    return configs

def benchmark_session(session, input_name: str, warmup_runs: int = 5, benchmark_runs: int = 50):
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
    # ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆ
    dummy_input = np.random.randn(1, 3, 640, 640).astype(np.float32)
    
    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    for _ in range(warmup_runs):
        _ = session.run(None, {input_name: dummy_input})
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    times = []
    for _ in range(benchmark_runs):
        start_time = time.perf_counter()
        outputs = session.run(None, {input_name: dummy_input})
        end_time = time.perf_counter()
        times.append((end_time - start_time) * 1000)  # ãƒŸãƒªç§’å¤‰æ›
    
    # çµ±è¨ˆè¨ˆç®—
    times = np.array(times)
    stats = {
        'mean': np.mean(times),
        'std': np.std(times),
        'min': np.min(times),
        'max': np.max(times),
        'median': np.median(times),
        'p95': np.percentile(times, 95),
        'p99': np.percentile(times, 99),
        'fps': 1000 / np.mean(times),
        'output_shapes': [out.shape for out in outputs]
    }
    
    return stats

def measure_memory_usage(session, input_name: str):
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š"""
    import psutil
    
    # å®Ÿè¡Œå‰ãƒ¡ãƒ¢ãƒªæ¸¬å®š
    process = psutil.Process()
    initial_memory = process.memory_info().rss / 1024 / 1024  # MB
    
    # ãƒ€ãƒŸãƒ¼æ¨è«–å®Ÿè¡Œ
    dummy_input = np.random.randn(1, 3, 640, 640).astype(np.float32)
    _ = session.run(None, {input_name: dummy_input})
    
    # å®Ÿè¡Œå¾Œãƒ¡ãƒ¢ãƒªæ¸¬å®š
    final_memory = process.memory_info().rss / 1024 / 1024  # MB
    
    return {
        'initial_memory_mb': initial_memory,
        'final_memory_mb': final_memory,
        'memory_increase_mb': final_memory - initial_memory
    }

def run_comprehensive_benchmark():
    """åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
    print("ğŸš€ ONNX Runtime åŒ…æ‹¬ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
    
    # ONNX ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
    onnx_path = project_root / "exports/onnx/yolov7n_mezzopiano.onnx"
    if not onnx_path.exists():
        print(f"âŒ ONNXãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {onnx_path}")
        return False
    
    print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«: {onnx_path}")
    print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {onnx_path.stat().st_size / 1024 / 1024:.1f} MB")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šä½œæˆ
    configs = create_session_configs()
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœä¿å­˜
    results = {}
    
    for config_name, config in configs.items():
        print(f"\n=== {config_name} ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ===")
        print(f"èª¬æ˜: {config['description']}")
        
        try:
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            session = ort.InferenceSession(
                str(onnx_path),
                sess_options=config['session_options'],
                providers=config['providers']
            )
            
            # å…¥åŠ›åå–å¾—
            input_name = session.get_inputs()[0].name
            
            # æ¨è«–é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            print("â±ï¸ æ¨è«–é€Ÿåº¦æ¸¬å®šä¸­...")
            perf_stats = benchmark_session(session, input_name)
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š
            print("ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šä¸­...")
            memory_stats = measure_memory_usage(session, input_name)
            
            # çµæœä¿å­˜
            results[config_name] = {
                'config': config,
                'performance': perf_stats,
                'memory': memory_stats,
                'providers_used': session.get_providers()
            }
            
            # çµæœè¡¨ç¤º
            print(f"âœ… å¹³å‡æ¨è«–æ™‚é–“: {perf_stats['mean']:.1f}ms")
            print(f"âœ… FPS: {perf_stats['fps']:.1f}")
            print(f"âœ… P95æ¨è«–æ™‚é–“: {perf_stats['p95']:.1f}ms")
            print(f"âœ… ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_stats['final_memory_mb']:.1f}MB")
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            del session
            gc.collect()
            
        except Exception as e:
            print(f"âŒ {config_name} ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¤±æ•—: {e}")
            results[config_name] = {'error': str(e)}
    
    return results

def generate_benchmark_report(results: Dict):
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    print(f"\n=== ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ ===")
    
    output_dir = project_root / "analysis/onnx_benchmark"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    report_path = output_dir / "benchmark_report.md"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# YOLOv7n ONNX Runtime ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"**ç”Ÿæˆæ—¥æ™‚**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**ãƒ¢ãƒ‡ãƒ«**: YOLOv7n Mezzopiano ONNX\n\n")
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        f.write("## ğŸ–¥ï¸ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±\n\n")
        f.write(f"- **CPU**: {psutil.cpu_count()}ã‚³ã‚¢\n")
        f.write(f"- **ãƒ¡ãƒ¢ãƒª**: {psutil.virtual_memory().total / 1024**3:.1f}GB\n")
        f.write(f"- **ONNX Runtime**: {ort.__version__}\n")
        f.write(f"- **åˆ©ç”¨å¯èƒ½ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼**: {', '.join(ort.get_available_providers())}\n\n")
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒ†ãƒ¼ãƒ–ãƒ«
        f.write("## ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ\n\n")
        f.write("| è¨­å®š | å¹³å‡æ¨è«–æ™‚é–“(ms) | FPS | P95æ™‚é–“(ms) | ãƒ¡ãƒ¢ãƒª(MB) | ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ |\n")
        f.write("|------|------------------|-----|-------------|------------|-------------|\n")
        
        for config_name, result in results.items():
            if 'error' in result:
                f.write(f"| {config_name} | âŒ ã‚¨ãƒ©ãƒ¼ | - | - | - | - |\n")
            else:
                perf = result['performance']
                memory = result['memory']
                providers = ', '.join(result['providers_used'])
                
                f.write(f"| {config_name} | {perf['mean']:.1f} | {perf['fps']:.1f} | {perf['p95']:.1f} | {memory['final_memory_mb']:.1f} | {providers} |\n")
        
        # è©³ç´°çµ±è¨ˆ
        f.write("\n## ğŸ“ˆ è©³ç´°çµ±è¨ˆ\n\n")
        
        best_fps = 0
        best_config = None
        
        for config_name, result in results.items():
            if 'error' not in result:
                perf = result['performance']
                memory = result['memory']
                
                f.write(f"### {config_name}\n")
                f.write(f"- **èª¬æ˜**: {result['config']['description']}\n")
                f.write(f"- **å¹³å‡æ¨è«–æ™‚é–“**: {perf['mean']:.2f}ms Â± {perf['std']:.2f}ms\n")
                f.write(f"- **æœ€å°/æœ€å¤§**: {perf['min']:.1f}ms / {perf['max']:.1f}ms\n")
                f.write(f"- **ä¸­å¤®å€¤**: {perf['median']:.1f}ms\n")
                f.write(f"- **P95/P99**: {perf['p95']:.1f}ms / {perf['p99']:.1f}ms\n")
                f.write(f"- **FPS**: {perf['fps']:.1f}\n")
                f.write(f"- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: {memory['final_memory_mb']:.1f}MB\n")
                f.write(f"- **å‡ºåŠ›å½¢çŠ¶**: {perf['output_shapes'][:3]}... (æœ€åˆã®3ã¤)\n\n")
                
                if perf['fps'] > best_fps:
                    best_fps = perf['fps']
                    best_config = config_name
        
        # æ¨å¥¨è¨­å®š
        f.write("## ğŸ¯ æ¨å¥¨è¨­å®š\n\n")
        if best_config:
            f.write(f"**æœ€é«˜æ€§èƒ½**: {best_config} ({best_fps:.1f} FPS)\n\n")
        
        # ç”¨é€”åˆ¥æ¨å¥¨
        f.write("### ç”¨é€”åˆ¥æ¨å¥¨\n")
        f.write("- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–**: CPUæœ€é©åŒ–ã¾ãŸã¯GPUè¨­å®š\n")
        f.write("- **ãƒãƒƒãƒå‡¦ç†**: ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š\n")
        f.write("- **çµ„ã¿è¾¼ã¿ç’°å¢ƒ**: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š\n\n")
    
    print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
    return report_path

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸ“Š ONNX Runtime ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ»æœ€é©åŒ–ãƒ„ãƒ¼ãƒ«")
    
    try:
        # åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        results = run_comprehensive_benchmark()
        
        if not results:
            return False
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_path = generate_benchmark_report(results)
        
        print(f"\nğŸ‰ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†!")
        print(f"ğŸ“ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)